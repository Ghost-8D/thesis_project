{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HFO_Mar15.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLNr9VVSbCQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r759K-obiQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "import operator\n",
        "\n",
        "def test(a,b,cmp,cname=None):\n",
        "    if cname is None: cname=cmp.__name__\n",
        "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
        "\n",
        "def test_eq(a,b): test(a,b,operator.eq,'==')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MP9le5mbiTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "from fastai import datasets\n",
        "import pickle, gzip, math, torch, matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import tensor\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXteObIKbiWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plus 10 data\n",
        "train_url = 'https://raw.githubusercontent.com/Ghost-8D/thesis_project/master/PSSP_Playground/datasetsPlus10/trainSet0.txt'\n",
        "test_url = 'https://raw.githubusercontent.com/Ghost-8D/thesis_project/master/PSSP_Playground/datasetsPlus10/testSet0.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub_JzN6ybiZV",
        "colab_type": "code",
        "outputId": "5e82d397-c8ef-4394-d06d-4639587d3da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! mkdir pssp_model\n",
        "! ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘pssp_model’: File exists\n",
            "pssp_model  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOK_s2LYbicZ",
        "colab_type": "code",
        "outputId": "f4fa900b-0fbf-4dfa-957d-d1530e58d431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = Path('/pssp_model')\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "path"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/pssp_model')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhsp8wajbifJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    df_train = pd.read_csv(train_url, header=None)\n",
        "    df_valid = pd.read_csv(test_url, header=None)\n",
        "    def get_XY_from_df(df):\n",
        "        x = df.values[:, :-1]\n",
        "        y = df.values[:, TOTAL_COLS-1]\n",
        "        return x, y\n",
        "    x_train, y_train = get_XY_from_df(df_train)\n",
        "    x_valid, y_valid = get_XY_from_df(df_valid)\n",
        "    x_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\n",
        "    return x_train.float(), y_train, x_valid.float(), y_valid\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfCiaGQ0bih2",
        "colab_type": "code",
        "outputId": "60d817fa-e586-4d5b-9b04-5bb5143f58d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "NEIGHBOURS = 10 # 1 amino-acid left and 1 amino-acid right added \n",
        "AMINO_ACID_LEN = 20\n",
        "WINDOW = 2 * NEIGHBOURS + 1\n",
        "TOTAL_AMINO_ACIDS = WINDOW * AMINO_ACID_LEN\n",
        "TOTAL_COLS = TOTAL_AMINO_ACIDS + 1 # plus the secondary structure category\n",
        "CATEGORIES = 3\n",
        "TOTAL_COLS"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYF7Ax5Bbikl",
        "colab_type": "code",
        "outputId": "f6915168-3513-4545-c408-abb84b088823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "x_train, y_train, x_valid, y_valid = get_data()\n",
        "n,c = x_train.shape\n",
        "x_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [18., 10., 64.,  ...,  0.,  0.,  0.]]),\n",
              " torch.Size([77092, 420]),\n",
              " tensor([0, 0, 1,  ..., 1, 0, 0]),\n",
              " torch.Size([77092]),\n",
              " tensor(0),\n",
              " tensor(2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuT8suKKbinc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert n==y_train.shape[0]==77092\n",
        "test_eq(c, WINDOW * AMINO_ACID_LEN)\n",
        "test_eq(y_train.min(),0)\n",
        "test_eq(y_train.max(),2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o7nK_YfbiqQ",
        "colab_type": "code",
        "outputId": "a75a0ffd-930c-4c48-d87e-a8b72a691405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([77092, 420]),\n",
              " torch.Size([77092]),\n",
              " torch.Size([7289, 420]),\n",
              " torch.Size([7289]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpbhdFK_bitG",
        "colab_type": "code",
        "outputId": "0a979e17-eb82-4b89-dd2b-343cf00867cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mpl.rcParams['image.cmap'] = 'cool'\n",
        "img = x_train[0]\n",
        "img.view(WINDOW, AMINO_ACID_LEN).type()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XQTu4HIbiv4",
        "colab_type": "code",
        "outputId": "d7e6ec5f-8c58-4711-d72a-76e16ac2ec2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.imshow(img.view((21,20)));"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAPPElEQVR4nO3df4wc5X3H8fcnBvoHoGJi44AxGKUW\nkhMVJ92aRNDKJMEYC8VJlaa2qtQBKtMIS0GKVNFWgQj+oaoIUmMEccCBRARomzhYigNYNBJBSogP\ny/yG2rUc4cOxTUyBNKmo4dM/dlwde7v23s7e7d49n5d02plnnp15Rvbndndu9vnKNhExs71v0AOI\niMmXoEcUIEGPKECCHlGABD2iACcMegDtaM4cs3DhoIcRMf3s3Ytfe02tzUMZdBYuhJGRQY8iYvpp\nNNo213rrLmmFpJcl7ZZ0fZvtvyfpwWr7k5IW1jleRPSm56BLmgXcDlwOLAbWSFrc0u1q4HXbfwDc\nBvxjr8eLiN7VeUVfCuy2vcf228ADwKqWPquAe6vlfwM+KWnc54eImFx1gj4feGXM+r6qrW0f20eA\nN4D31zhmRPRgaP68JmmdpBFJIxw6NOjhRMwodYI+CiwYs3521da2j6QTgN8Hft1uZ7Y32m7YbjB3\nbo1hRUSrOkHfDiySdJ6kk4DVwJaWPluAtdXy54B/d74uFzHlev47uu0jktYDjwCzgE22n5d0EzBi\newtwN/BdSbuBwzR/GUTEFKt1w4ztrcDWlrYbxiz/D/DndY4REfUNzcW4iJg8CXpEARL0iAIk6BEF\nSNAjCpCgRxQgQY8oQIIeUYAEPaIACXpEARL0iAIk6BEFSNAjCpCgRxQgQY8oQIIeUYAEPaIAdQo4\nLJD0E0kvSHpe0pfb9Fkm6Q1JO6ufG9rtKyImV52ppI4AX7G9Q9KpwFOSttl+oaXfT21fUeM4EVFT\nz6/otvfb3lEtvwW8yPgCDhExBPryGb0qnvgR4Mk2mz8u6WlJP5b0oWPsIwUcIiZJ7aBLOgX4PnCd\n7TdbNu8AzrV9AfAN4Ied9pMCDhGTp27Z5BNphvw+2z9o3W77Tdu/qZa3AidKmlPnmBExcXWuuotm\ngYYXbX+9Q58PHK2eKmlpdby2JZkiYvLUuep+EfAF4FlJO6u2vwfOAbB9J80yTF+SdAT4HbA6JZki\npl6dkkxPAMesdW57A7Ch12NERH/kzriIAiToEQVI0CMKkKBHFCBBjyhAgh5RgAQ9ogAJekQBEvSI\nAiToEQVI0CMKkKBHFCBBjyhAgh5RgAQ9ogAJekQBEvSIAvRjFti9kp6tKrGMtNkuSf8sabekZyR9\ntO4xI2Ji6swZN9Yltl/rsO1yYFH1cyFwR/UYEVNkKt66rwK+46afA6dJOnMKjhsRlX4E3cCjkp6S\ntK7N9vnAK2PW99GmdFMqtURMnn68db/Y9qikM4Btkl6y/fhEd2J7I7ARQI1GpoSO6KPar+i2R6vH\ng8BmYGlLl1FgwZj1s6u2iJgidUsynVyVTEbSycBy4LmWbluAv6quvn8MeMP2/jrHjYiJqfvWfR6w\nuaq6dALwPdsPS/ob+P9qLVuBlcBu4LfAlTWPGRETVCvotvcAF7Rpv3PMsoFr6xwnIurJnXERBUjQ\nIwqQoEcUIEGPKECCHlGABD2iAAl6RAES9IgCJOgRBUjQIwqQoEcUIEGPKECCHlGABD2iAAl6RAES\n9IgCJOgRBeg56JLOr6qzHP15U9J1LX2WSXpjTJ8b6g85Iiaq56mkbL8MLAGQNIvmzK6b23T9qe0r\nej1ORNTXr7funwT+0/Yv+7S/iOijfgV9NXB/h20fl/S0pB9L+lCnHaRSS8Tk6Uc11ZOATwP/2mbz\nDuBc2xcA3wB+2Gk/tjfabthuMHdu3WFFxBj9eEW/HNhh+0DrBttv2v5NtbwVOFHSnD4cMyImoB9B\nX0OHt+2SPqCquoOkpdXxft2HY0bEBNQq4FCVYboUuGZM29gqLZ8DviTpCPA7YHVV0CEiplDdSi3/\nDby/pW1slZYNwIY6x4iI+nJnXEQBEvSIAiToEQVI0CMKkKBHFCBBjyhAgh5RgAQ9ogAJekQBEvSI\nAiToEQVI0CMKkKBHFCBBjyhAgh5RgFrfR4/hZ3XfV5kSZMbKK3pEAboKuqRNkg5Kem5M2+mStkna\nVT3O7vDctVWfXZLW9mvgEdG9bl/R7wFWtLRdDzxmexHwWLX+HpJOB24ELgSWAjd2+oUQEZOnq6Db\nfhw43NK8Cri3Wr4X+Eybp14GbLN92PbrwDbG/8KIiElW5zP6PNv7q+VfAfPa9JkPvDJmfV/VNk4q\ntURMnr5cjKumcK51zTaVWiImT52gH5B0JkD1eLBNn1FgwZj1s6u2iJhCdYK+BTh6FX0t8FCbPo8A\nyyXNri7CLa/aImIKdfvntfuBnwHnS9on6WrgFuBSSbuAT1XrSGpIugvA9mHgZmB79XNT1RYRU0jD\nWCFJjYYZGRn0MIrT7V10uYNuiDUaeGRk3L9k7oyLKECCHlGABD2iAAl6RAES9IgCJOgRBUjQIwqQ\noEcUIEGPKECCHlGATA45TTW2d9fv8oe732e/b21d1e5rTh08tKq/x473yit6RAES9IgCJOgRBUjQ\nIwqQoEcUIEGPKMBxg96hSss/SXpJ0jOSNks6rcNz90p6VtJOSZkyJmJAunlFv4fxRRe2AR+2/YfA\nfwB/d4znX2J7ie1Gb0OMiLqOG/R2VVpsP2r7SLX6c5rTOEfEkOrHnXFXAQ922GbgUUkGvml7Y6ed\nSFoHrAM4ZfY5rN1w/APfvn7CY+2ry7qcuPqRy/p/7JE/7q7f/Ff7f+xu5W634VEr6JL+ATgC3Neh\ny8W2RyWdAWyT9FL1DmGc6pfARoAzzmlkntGIPur5qrukLwJXAH/pDnNG2x6tHg8Cm2lWVI2IKdZT\n0CWtAP4W+LTt33boc7KkU48u06zS8ly7vhExubr581q7Ki0bgFNpvh3fKenOqu9ZkrZWT50HPCHp\naeAXwI9sT+C7VBHRL8f9jG57TZvmuzv0fRVYWS3vAS6oNbqI6IvcGRdRgAQ9ogAJekQBEvSIAqRs\nckzYV2/urt+J/9v9Pm+4qbexRIuUTY4oV4IeUYAEPaIACXpEARL0iAIk6BEFSNAjCpCgRxQgQY8o\nQIIeUYChLJv8R0/B+Jv4xut3md/ozs1fHfQIYqLyih5RgF4rtXxN0mg1jdROSSs7PHeFpJcl7ZZ0\nfT8HHhHd67VSC8BtVQWWJba3tm6UNAu4HbgcWAyskbS4zmAjojc9VWrp0lJgt+09tt8GHgAypX/E\nANT5jL6+KrK4SdLsNtvnA6+MWd9XtbUlaZ2kEUkjhzhUY1gR0arXoN8BfBBYAuwHbq07ENsbbTds\nN+Yyt+7uImKMnoJu+4Dtd2y/C3yL9hVYRoEFY9bPrtoiYor1WqnlzDGrn6V9BZbtwCJJ50k6CVgN\nbOnleBFRz3FvmKkqtSwD5kjaB9wILJO0hGa11L3ANVXfs4C7bK+0fUTSeuARYBawyfbzk3IWEXFM\nmRwyJiyTQw6xTA4ZUa4EPaIACXpEARL0iAIk6BEFSNAjCpCgRxQgQY8oQIIeUYChnDMuBsNdzNMH\nmatvOsorekQBEvSIAiToEQVI0CMKkKBHFCBBjyhAgh5RgG6mktoEXAEctP3hqu1B4Pyqy2nAf9le\n0ua5e4G3gHeAI7YbfRp3RExANzfM3ANsAL5ztMH2XxxdlnQr8MYxnn+J7dd6HWBE1HfcoNt+XNLC\ndtskCfg88In+Disi+qnuLbB/AhywvavDdgOPSjLwTdsbO+1I0jpgHQDnnFNzWHHUpqu679vtra1X\nfru7ft++svtjx+SqG/Q1wP3H2H6x7VFJZwDbJL1U1XIbp/olsBGqWWAjom96vuou6QTgz4AHO/Wx\nPVo9HgQ2076iS0RMsjp/XvsU8JLtfe02SjpZ0qlHl4HltK/oEhGT7LhBryq1/Aw4X9I+SVdXm1bT\n8rZd0lmSjtZKnwc8Ielp4BfAj2w/3L+hR0S3urnqvqZD+xfbtL0KrKyW9wAX1BxfRPRB7oyLKECC\nHlGABD2iAAl6RAEyOeQMd9Wm/u/zoie66/e+d7vf591XH79P9C6v6BEFSNAjCpCgRxQgQY8oQIIe\nUYAEPaIACXpEARL0iAIk6BEFSNAjCpBbYKepQdYy/+u7+7/PmFzdzDCzQNJPJL0g6XlJX67aT5e0\nTdKu6nF2h+evrfrskrS23ycQEcfXzVv3I8BXbC8GPgZcK2kxcD3wmO1FwGPV+ntIOh24EbiQ5sSQ\nN3b6hRARk+e4Qbe93/aOavkt4EVgPrAKuLfqdi/wmTZPvwzYZvuw7deBbcCKfgw8Iro3oYtxVcWW\njwBPAvNs7682/YrmZJCt5gOvjFnfV7VFxBTqOuiSTgG+D1xn+82x22ybZlWWnklaJ2lE0giHDtXZ\nVUS06Crokk6kGfL7bP+gaj4g6cxq+5nAwTZPHQUWjFk/u2obx/ZG2w3bDebO7Xb8EdGFbq66C7gb\neNH218ds2gIcvYq+FniozdMfAZZLml1dhFtetUXEFOrmFf0i4AvAJyTtrH5WArcAl0raRbNqyy0A\nkhqS7gKwfRi4Gdhe/dxUtUXEFFLz4/VwUaNhRkYGPYyhNsgbZmKINRp4ZGTc/47hDLp0CPhlS/Mc\n4LUBDGeyzKTzmUnnAtP7fM61Pe4i11AGvR1JI7Ybgx5Hv8yk85lJ5wIz73wgX2qJKEKCHlGA6RT0\njYMeQJ/NpPOZSecCM+98ps9n9Ijo3XR6RY+IHiXoEQUY+qBLWiHpZUm7JY37zvt0I2mvpGerOwyn\n3V1BkjZJOijpuTFtXU1CMow6nM/XJI223Ak6rQ110CXNAm4HLgcWA2uqSS+mu0tsL5mmf6u9h/Fz\nChx3EpIhdg/t50i4rfo3WmJ76xSPqe+GOug0Z6XZbXuP7beBB2hOeBEDYvtxoPX7Ct1MQjKUOpzP\njDPsQZ+JE1cYeFTSU5LWDXowfdLNJCTTzXpJz1Rv7afNR5FOhj3oM9HFtj9K8+PItZL+dNAD6qd+\nTEIyBO4APggsAfYDtw52OPUNe9C7nrhiurA9Wj0eBDbT/Hgy3XUzCcm0YfuA7Xdsvwt8ixnwbzTs\nQd8OLJJ0nqSTgNU0J7yYliSdLOnUo8s0J+J47tjPmha6mYRk2jj6S6vyWWbAv9FQF3CwfUTSepqz\n0swCNtl+fsDDqmMesLk5aQ8nAN+z/fBghzQxku4HlgFzJO2jOZ33LcC/SLqa5teLPz+4EU5Mh/NZ\nJmkJzY8ge4FrBjbAPsktsBEFGPa37hHRBwl6RAES9IgCJOgRBUjQIwqQoEcUIEGPKMD/AT4PdUTl\n05O0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJAWVlPibiyy",
        "colab_type": "code",
        "outputId": "adf802b3-ba07-4a09-8e1e-3bb6625d4a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_mean, train_std = x_train.mean(), x_train.std()\n",
        "train_mean, train_std"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(4.7987), tensor(11.2503))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwDtY1c0cE6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT2I4GQGcE-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = normalize(x_train, train_mean, train_std)\n",
        "# NOTE: Use training, not validation mean for validation set\n",
        "x_valid = normalize(x_valid, train_mean, train_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbyYxfMicFCC",
        "colab_type": "code",
        "outputId": "8f299664-aa00-4bd5-85f3-f2865553c6be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_mean, train_std = x_train.mean(), x_train.std()\n",
        "train_mean, train_std"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5.2017e-05), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1veWQtEcFFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuxMCHiqcFIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Module():\n",
        "    def __call__(self, *args):\n",
        "        self.args = args\n",
        "        self.out = self.forward(*args)\n",
        "        return self.out\n",
        "    \n",
        "    def forward(self): raise Exception('not implemented')\n",
        "    def backward(self): self.bwd(self.out, *self.args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReqApxWacFLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lin(Module):\n",
        "    def __init__(self, w, b): \n",
        "        super().__init__()\n",
        "        self.w,self.b = w,b\n",
        "    \n",
        "    def forward(self, inp): return inp@self.w + self.b\n",
        "    \n",
        "    def bwd(self, out, inp):\n",
        "        inp.g = out.g @ self.w.t()\n",
        "        self.w.g = inp.t() @ out.g\n",
        "        self.b.g = out.g.sum(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PleUZK1ocFON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu(Module):\n",
        "    def __init__(self): super().__init__()\n",
        "    def forward(self, inp): return torch.clamp(inp, 0, 1e10) #return inp.clamp_min(0.)-0.5\n",
        "    def bwd(self, out, inp): return out == inp #inp.g = (inp>0).float() * out.g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2taNtADcFRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model():\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        self.layers = nn.ModuleList([Lin(n_in, nh), Relu(), Lin(nh, n_out)])\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def __call__(self, x, targ):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return self.loss(x, targ)\n",
        "    \n",
        "    def backward(self):\n",
        "        self.loss.backward()\n",
        "        for l in reversed(self.layers): l.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV2bNXsLcFUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out), nn.ReLU()]\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.params = [torch.randn(n_in, nh) * math.sqrt(2.0/n_in),\n",
        "                           torch.randn(nh),\n",
        "                           torch.randn(nh, n_out) * math.sqrt(2.0/nh),\n",
        "                           torch.randn(n_out)]\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        self.activations = []\n",
        "        self.d_activations = []\n",
        "        for l in self.layers: \n",
        "            out_x = l(x)\n",
        "            if (isinstance(l, type(nn.ReLU()))):\n",
        "                self.activations.append(x)\n",
        "                self.d_activations.append(x == out_x)\n",
        "            x = out_x\n",
        "        return x \n",
        "\n",
        "    def parameters(self):\n",
        "        return self.params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1aHOmplcdFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXu926bTcdIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
        "        \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for p in self.params: p -= p.grad * lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.params: p.grad.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLeBnnUYcdLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i],self.y[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSfAqeDcdOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdBXMkIvcdRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, c=None):\n",
        "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
        "        \n",
        "    @property\n",
        "    def train_ds(self): return self.train_dl.dataset\n",
        "        \n",
        "    @property\n",
        "    def valid_ds(self): return self.valid_dl.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA32Xh0VcdUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "assert len(train_ds)==len(x_train)\n",
        "assert len(valid_ds)==len(x_valid)\n",
        "nh,bs = 50, 78000\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJuodP7tcdXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWM7tz1ycufh",
        "colab_type": "code",
        "outputId": "2d6b9bff-0a9d-41f6-998b-3b526ab0b76c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = nn.Sequential(nn.Linear(420,50), nn.ReLU(), nn.Linear(50,3))\n",
        "model.parameters()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7fa32262e150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-409B9TCcuiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(data, lr=0.5, nh=50):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c)) #Model(m, nh, data.c) \n",
        "    return model, optim.SGD(model.parameters(), lr=lr) #HessianFree(model.parameters(), CG_iter=100, init_damping=20, net=model)\n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A33rrv0pcumE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(*get_model(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSRr7Q7acupj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, learn):\n",
        "    for epoch in range(epochs):\n",
        "        learn.model.train()\n",
        "        for xb,yb in learn.data.train_dl:\n",
        "            loss = learn.loss_func(learn.model(xb), yb)\n",
        "            loss.backward()\n",
        "            learn.opt.step()\n",
        "            learn.opt.zero_grad()\n",
        "\n",
        "        learn.model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc = 0.0, 0.0\n",
        "            for xb,yb in learn.data.valid_dl:\n",
        "                pred = learn.model(xb)\n",
        "                tot_loss += learn.loss_func(pred, yb)\n",
        "                tot_acc  += accuracy (pred,yb)\n",
        "        nv = len(learn.data.valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "    return tot_loss/nv, tot_acc/nv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjboABr3cuso",
        "colab_type": "code",
        "outputId": "e0945354-4c71-4a0f-fd62-bdb9d5582d33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "loss,acc = fit(10, learn)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(11.9970) tensor(0.3308)\n",
            "1 tensor(18.8337) tensor(0.4276)\n",
            "2 tensor(1.1143) tensor(0.2479)\n",
            "3 tensor(1.0906) tensor(0.4232)\n",
            "4 tensor(1.0811) tensor(0.4269)\n",
            "5 tensor(1.0662) tensor(0.4383)\n",
            "6 tensor(1.0445) tensor(0.4514)\n",
            "7 tensor(1.0703) tensor(0.4267)\n",
            "8 tensor(1.0523) tensor(0.4349)\n",
            "9 tensor(1.1122) tensor(0.4073)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_r6flpcuwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
        "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
        "def camel2snake(name):\n",
        "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
        "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
        "\n",
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run=run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "    @property\n",
        "    def name(self):\n",
        "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
        "        return camel2snake(name or 'callback')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szwr-biXcdbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainEvalCallback(Callback):\n",
        "    def begin_fit(self):\n",
        "        self.run.n_epochs=0.\n",
        "        self.run.n_iter=0\n",
        "    \n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.run.n_epochs += 1./self.iters\n",
        "        self.run.n_iter   += 1\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.run.n_epochs=self.epoch\n",
        "        self.model.train()\n",
        "        self.run.in_train=True\n",
        "\n",
        "    def begin_validate(self):\n",
        "        self.model.eval()\n",
        "        self.run.in_train=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stz48NH9dEoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import *\n",
        "from functools import partial\n",
        "\n",
        "def listify(o):\n",
        "    if o is None: return []\n",
        "    if isinstance(o, list): return o\n",
        "    if isinstance(o, str): return [o]\n",
        "    if isinstance(o, Iterable): return list(o)\n",
        "    return [o]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l9D6BovdEsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Runner():\n",
        "    def __init__(self, cbs=None, cb_funcs=None):\n",
        "        cbs = listify(cbs)\n",
        "        for cbf in listify(cb_funcs):\n",
        "            cb = cbf()\n",
        "            setattr(self, cb.name, cb)\n",
        "            cbs.append(cb)\n",
        "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
        "\n",
        "    @property\n",
        "    def opt(self):       return self.learn.opt\n",
        "    @property\n",
        "    def model(self):     return self.learn.model\n",
        "    @property\n",
        "    def loss_func(self): return self.learn.loss_func\n",
        "    @property\n",
        "    def data(self):      return self.learn.data\n",
        "\n",
        "    def one_batch(self, xb, yb):\n",
        "        self.xb,self.yb = xb,yb\n",
        "        if self('begin_batch'): return\n",
        "        self.pred = self.model(self.xb)\n",
        "        if self('after_pred'): return\n",
        "        self.loss = self.loss_func(self.pred, self.yb)\n",
        "        if self('after_loss') or not self.in_train: return\n",
        "        self.loss.backward()\n",
        "        if self('after_backward'): return\n",
        "        self.opt.step()\n",
        "        if self('after_step'): return\n",
        "        self.opt.zero_grad()\n",
        "\n",
        "    def all_batches(self, dl):\n",
        "        self.iters = len(dl)\n",
        "        for xb,yb in dl:\n",
        "            if self.stop: break\n",
        "            self.one_batch(xb, yb)\n",
        "            self('after_batch')\n",
        "        self.stop=False\n",
        "\n",
        "    def fit(self, epochs, learn):\n",
        "        self.epochs,self.learn = epochs,learn\n",
        "\n",
        "        try:\n",
        "            for cb in self.cbs: cb.set_runner(self)\n",
        "            if self('begin_fit'): return\n",
        "            for epoch in range(epochs):\n",
        "                self.epoch = epoch\n",
        "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "\n",
        "                with torch.no_grad(): \n",
        "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "                if self('after_epoch'): break\n",
        "            \n",
        "        finally:\n",
        "            self('after_fit')\n",
        "            self.learn = None\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
        "            f = getattr(cb, cb_name, None)\n",
        "            if f and f(): return True\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74UDlpVcdEvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AvgStats():\n",
        "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
        "    \n",
        "    def reset(self):\n",
        "        self.tot_loss,self.count = 0.,0\n",
        "        self.tot_mets = [0.] * len(self.metrics)\n",
        "        \n",
        "    @property\n",
        "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
        "    @property\n",
        "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        if not self.count: return \"\"\n",
        "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
        "\n",
        "    def accumulate(self, run):\n",
        "        bn = run.xb.shape[0]\n",
        "        self.tot_loss += run.loss * bn\n",
        "        self.count += bn\n",
        "        for i,m in enumerate(self.metrics):\n",
        "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
        "\n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "        \n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "    \n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxB1QGtOdEyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_learner(model_func, loss_func, data):\n",
        "    return Learner(*model_func(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpJP_1nndE1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_func(lr=0.5): return partial(get_model, lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bacyws7udSsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Recorder(Callback):\n",
        "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.lrs.append(self.opt.param_groups[-1]['lr'])\n",
        "        self.losses.append(self.loss.detach().cpu())        \n",
        "\n",
        "    def plot_lr  (self): plt.plot(self.lrs)\n",
        "    def plot_loss(self): plt.plot(self.losses)\n",
        "\n",
        "class ParamScheduler(Callback):\n",
        "    _order=1\n",
        "    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func\n",
        "\n",
        "    def set_param(self):\n",
        "        for pg in self.opt.param_groups:\n",
        "            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)\n",
        "            \n",
        "    def begin_batch(self): \n",
        "        if self.in_train: self.set_param()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmwd8ke0dSvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annealer(f):\n",
        "    def _inner(start, end): return partial(f, start, end)\n",
        "    return _inner\n",
        "\n",
        "@annealer\n",
        "def sched_lin(start, end, pos): return start + pos*(end-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DDCWRNHdSyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@annealer\n",
        "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
        "@annealer\n",
        "def sched_no(start, end, pos):  return start\n",
        "@annealer\n",
        "def sched_exp(start, end, pos): return start * (end/start) ** pos\n",
        "\n",
        "def cos_1cycle_anneal(start, high, end):\n",
        "    return [sched_cos(start, high), sched_cos(high, end)]\n",
        "\n",
        "#This monkey-patch is there to be able to plot tensors\n",
        "torch.Tensor.ndim = property(lambda x: len(x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM-EMA9sdS1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_scheds(pcts, scheds):\n",
        "    assert sum(pcts) == 1.\n",
        "    pcts = tensor([0] + listify(pcts))\n",
        "    assert torch.all(pcts >= 0)\n",
        "    pcts = torch.cumsum(pcts, 0)\n",
        "    def _inner(pos):\n",
        "        idx = (pos >= pcts).nonzero().max()\n",
        "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
        "        return scheds[idx](actual_pos)\n",
        "    return _inner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBdiKCUIdS5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_to(train, valid):\n",
        "    m,s = train.mean(),train.std()\n",
        "    return normalize(train, m, s), normalize(valid, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1W27uIAdgRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x): return self.func(x)\n",
        "\n",
        "def flatten(x):      return x.view(x.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sowOzwbPdgUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cnn_model(data):\n",
        "    return nn.Sequential(\n",
        "        Lambda(pssp_resize),\n",
        "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14\n",
        "        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7\n",
        "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
        "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        Lambda(flatten),\n",
        "        nn.Linear(32,data.c)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57kvTSQldga6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CudaCallback(Callback):\n",
        "    def begin_fit(self): self.model.cuda()\n",
        "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwIuvXZ1dggz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchTransformXCallback(Callback):\n",
        "    _order=2\n",
        "    def __init__(self, tfm): self.tfm = tfm\n",
        "    def begin_batch(self): self.run.xb = self.tfm(self.xb)\n",
        "\n",
        "def view_tfm(*size):\n",
        "    def _inner(x): return x.view(*((-1,)+size))\n",
        "    return _inner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yuz6sOwdgkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cnn_layers(data, nfs):\n",
        "    nfs = [1] + nfs\n",
        "    return [\n",
        "        conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3)\n",
        "        for i in range(len(nfs)-1)\n",
        "    ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]\n",
        "\n",
        "def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2kRzN8Ddgov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(ni, nf, ks=3, stride=2):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8buGHaUIdgs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):\n",
        "    if opt_func is None: opt_func = optim.SGD\n",
        "    opt = opt_func(model.parameters(), lr=lr)\n",
        "    learn = Learner(model, opt, loss_func, data)\n",
        "    return learn, Runner(cb_funcs=listify(cbs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoW_NR-IdgfL",
        "colab_type": "code",
        "outputId": "a3bb5f78-fe6d-47ce-a9ca-7a14e24f5742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "cbfs = [Recorder, partial(AvgStatsCallback,accuracy)]\n",
        "cbfs.append(CudaCallback)\n",
        "pssp_view = view_tfm(1,21,20)\n",
        "cbfs.append(partial(BatchTransformXCallback, pssp_view))\n",
        "nfs = [8,16,32,32]\n",
        "model = get_cnn_model(data, nfs)\n",
        "learn,run = get_runner(model, data, lr=0.4, cbs=cbfs)\n",
        "model"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (4): AdaptiveAvgPool2d(output_size=1)\n",
              "  (5): Lambda()\n",
              "  (6): Linear(in_features=32, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beCjgQizdgXu",
        "colab_type": "code",
        "outputId": "69eebe33-0f21-4710-fb87-df9a1a228f7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "run.fit(3, learn)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: [1.086500240783739, tensor(0.4254, device='cuda:0')]\n",
            "valid: [1.074203342579572, tensor(0.4287, device='cuda:0')]\n",
            "train: [1.0703059129027654, tensor(0.4260, device='cuda:0')]\n",
            "valid: [1.070061359047023, tensor(0.4278, device='cuda:0')]\n",
            "train: [1.0648196716261091, tensor(0.4255, device='cuda:0')]\n",
            "valid: [1.0677507818930922, tensor(0.4278, device='cuda:0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiCqTDWqd9YU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_x_train = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "toy_y_train = np.array([0, 1, 1, 0])\n",
        "toy_x_valid = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "toy_y_valid = np.array([0, 1, 1, 0])\n",
        "toy_x_train, toy_y_train, toy_x_valid, toy_y_valid = torch.from_numpy(toy_x_train).float(), torch.from_numpy(toy_y_train), torch.from_numpy(toy_x_valid).float(), torch.from_numpy(toy_y_valid)\n",
        "toy_train_ds,toy_valid_ds = Dataset(toy_x_train, toy_y_train),Dataset(toy_x_valid, toy_y_valid)\n",
        "toy_nh, toy_bs = 120, 4\n",
        "toy_c = toy_y_train.max().item()+1\n",
        "loss_func = F.cross_entropy\n",
        "toy_data = DataBunch(*get_dls(toy_train_ds, toy_valid_ds, toy_bs), toy_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV9d_eBjd9bY",
        "colab_type": "code",
        "outputId": "7a57d55c-fad6-41ce-845b-cfaaae9a973f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "new_y_train = []\n",
        "for i in range(0, len(y_train)):\n",
        "    temp = [0, 0, 0]\n",
        "    temp[y_train[i].item()] = 1\n",
        "    new_y_train.append(temp)\n",
        "new_y_train = np.array(new_y_train)\n",
        "new_y_train = torch.from_numpy(new_y_train)\n",
        "new_y_train, y_train"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1, 0, 0],\n",
              "         [1, 0, 0],\n",
              "         [0, 1, 0],\n",
              "         ...,\n",
              "         [0, 1, 0],\n",
              "         [1, 0, 0],\n",
              "         [1, 0, 0]]), tensor([0, 0, 1,  ..., 1, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wee7OjBZd9hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss(output, targets):\n",
        "    expand_targets = torch.zeros_like(output)\n",
        "    for i in range(0, len(expand_targets)): expand_targets[i][targets[i]] = 1\n",
        "    assert output.shape[0] == 4\n",
        "    # return torch.sum((output - expand_targets) * (output - expand_targets)) / (2.0 * output.shape[0])\n",
        "    # temp = -np.sum(np.nan_to_num(targets) * np.log(1e-15 + output[-1]))\n",
        "    temp = -torch.sum(expand_targets * torch.log(1e-8 + output))\n",
        "    # temp = torch.nn.functional.cross_entropy(output[-1] + 1e-8, targets)\n",
        "    # print(\"loss\",-torch.sum(targets * torch.log(1e-8 + output[-1]), 1).mean())\n",
        "    # print(temp)\n",
        "    # print(\"cross entropy\", torch.nn.functional.cross_entropy(output[-1], torch.argmax(targets, dim=1)))\n",
        "    # print(\"acc:\", calc_accuracy(output[-1], targets))\n",
        "    return temp\n",
        "\n",
        "def d_loss(output, targets):\n",
        "    expand_targets = torch.zeros_like(output)\n",
        "    for i in range(0, len(expand_targets)): expand_targets[i][targets[i]] = 1\n",
        "    # return (output - expand_targets)\n",
        "    return -(expand_targets) / (1e-8 + output)\n",
        "\n",
        "    temp = []\n",
        "    for o in output:\n",
        "        temp.append(torch.zeros_like(o))            #temp.append(np.zeros_like(o))\n",
        "    # temp[-1] = -targets[torch.argmax(targets, dim=1)] / (1e-8 + output[-1])\n",
        "    temp[-1] = -targets / (1e-8 + output[-1])       #output[-1]\n",
        "    # print(temp[-1])\n",
        "    return temp\n",
        "\n",
        "def d2_loss(output, targets):\n",
        "    # return torch.ones_like(output)\n",
        "    # print(output.shape, targets.shape)\n",
        "    return torch.sum(torch.diag_embed(output, offset=0, dim1=-2, dim2=-1) - \n",
        "                     output.unsqueeze(-1) @ output.unsqueeze(1), 1)\n",
        "    # print(output.unsqueeze(-1).shape, output.unsqueeze(1).shape)\n",
        "    # print(output.shape, targets.shape)\n",
        "    # print(torch.diag_embed(output, offset=0, dim1=-2, dim2=-1))\n",
        "    # print((output.unsqueeze(-1) @ output.unsqueeze(1)).shape)\n",
        "    # print(targets / (output ** 2))\n",
        "    expand_targets = torch.zeros_like(output)\n",
        "    for i in range(0, len(expand_targets)): expand_targets[i][targets[i]] = 1\n",
        "    # print(expand_targets + 1e-8 / ((output) ** 2 + 1e-8))\n",
        "    return (expand_targets / ((output) ** 2 + 1e-8)).t()\n",
        "\n",
        "    return torch.diag_embed(output, offset=0, dim1=-2, dim2=-1) - output.unsqueeze(-1) @ output.unsqueeze(1)\n",
        "    # print(torch.diag(output.squeeze()).shape, output.t().shape, output.shape)\n",
        "    return torch.diag(output.squeeze()) - output.t() @ output\n",
        "    # print(output.shape, targets.shape)\n",
        "    # temp = []\n",
        "    # for i in output:\n",
        "    #     temp.append(torch.diag(output[o]) - output[o] @ output[o].t())\n",
        "    # return temp\n",
        "    return new_y_train / ((1e-5 + output) ** 2)\n",
        "    # temp = []\n",
        "    # for o in output:\n",
        "    #     temp.append(torch.zeros_like(o))                  #temp.append(np.zeros_like(o))\n",
        "    # # print(targets[torch.argmax(targets, dim=1)] / (1e-8 + output[-1]) ** 2)\n",
        "    # # temp[-1] = targets[torch.argmax(targets, dim=1)] / (1e-8 + output[-1]) ** 2\n",
        "    # temp[-1] = targets / ((1e-8 + output[-1]) ** 2)         #output[-1] ** 2\n",
        "    # # print(temp[-1])\n",
        "    # return temp\n",
        "\n",
        "def make_flat_v1(tensor_list):\n",
        "    temp = []\n",
        "    for c in range(0, len(tensor_list), 2):\n",
        "        temp.append(tensor_list[c].t().flatten())\n",
        "    for c in range(1, len(tensor_list), 2):\n",
        "        temp.append(tensor_list[c].t().flatten())\n",
        "    return torch.cat(temp, dim=0)\n",
        "\n",
        "def make_flat(tensor_list):\n",
        "    temp = []\n",
        "    for c in range(0, len(tensor_list)):\n",
        "        temp.append(tensor_list[c].view(-1))\n",
        "    return torch.cat(temp, dim=0)\n",
        "\n",
        "def calc_accuracy(out, yb): \n",
        "    # print(out.shape, yb.shape)\n",
        "    # print(out[:10])\n",
        "    # print(yb[:10])\n",
        "    return (torch.argmax(out, dim=1)==torch.argmax(yb, dim=1)).float().mean()\n",
        "\n",
        "def inner_product(arr1, arr2):\n",
        "    if len(arr1) != len(arr2): return None\n",
        "    product = 0.0\n",
        "    for i in range(0, len(arr1)):\n",
        "        product += torch.matmul(arr1[i].view(-1), arr2[i].view(-1))\n",
        "    return product\n",
        "\n",
        "def J_dot(J, vec, transpose_J=False, out=None):\n",
        "    \"\"\"Compute the product of a Jacobian and some vector.\"\"\"\n",
        "\n",
        "    if J.ndim == 2:\n",
        "        # note: the first dimension is the batch, so ndim==2 means\n",
        "        # this is a vector representation\n",
        "        if out is None:\n",
        "            # passing out=None fails for some reason\n",
        "            return torch.mul(J, vec) #np.multiply(J, vec)\n",
        "        else:\n",
        "            return torch.mul(J, vec, out=out) #np.multiply(J, vec, out=out)\n",
        "    else:\n",
        "        if transpose_J:\n",
        "            J = torch.transpose(J, 2, 1) #np.transpose(J, (0, 2, 1))\n",
        "\n",
        "        if out is None:\n",
        "            # passing out=None fails for some reason\n",
        "            return torch.einsum(\"ijk,ik->ij\", J, vec) \n",
        "            #np.einsum(\"ijk,ik->ij\", J, vec)\n",
        "\n",
        "        if out is vec:\n",
        "            tmp_vec = vec.copy()\n",
        "        else:\n",
        "            tmp_vec = vec\n",
        "\n",
        "        return torch.einsum(\"ijk,ik->ij\", J, tmp_vec, out=out) \n",
        "        #np.einsum(\"ijk,ik->ij\", J, tmp_vec, out=out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9NVCP9Ad9jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "class HessianFree(Optimizer):\n",
        "    \"\"\"Implements Hessian Free Optimisation algorithm.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        CG_iter (int): maximum number of CG iterations to run per epoch\n",
        "        init_damping (float): the initial value of the Tikhonov damping\n",
        "        net (nn.Module): the network that uses this optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, CG_iter=250, init_damping=1, net=None):\n",
        "        if not 0.0 <= CG_iter:\n",
        "            raise ValueError(\"Invalid number of CG iterations: {}\".format(cg_iter))\n",
        "        if not 0.0 <= init_damping:\n",
        "            raise ValueError(\"Invalid Tikhonov damping value: {}\".format(init_damping))\n",
        "        if net == None:\n",
        "            raise ValueError(\"Invalid net value: {}\".format(net))\n",
        "        \n",
        "        defaults = dict(CG_iter=CG_iter, damping=init_damping, net=net)\n",
        "        super(HessianFree, self).__init__(params, defaults)\n",
        "        \n",
        "        self.CG_iter = CG_iter\n",
        "        self.damping = init_damping\n",
        "        self.net = net\n",
        "        self.init_delta = None\n",
        "        self.params = None\n",
        "\n",
        "    # def grad_params(self):\n",
        "    #     return [p for pg in self.param_groups\n",
        "    #         for p in pg if p.grad is not None]\n",
        "\n",
        "    # def zero_grad(self):\n",
        "    #     for p in self.grad_params():\n",
        "    #         p.grad.detach_()\n",
        "    #         p.grad.zero_()\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        err = closure()  # note: don't reuse previous error (diff batch)\n",
        "\n",
        "        # compute gradient\n",
        "        params = []\n",
        "        grads = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                params.append(p.data)\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grads.append(p.grad.data) \n",
        "        self.params = params\n",
        "        # print(grads)\n",
        "        # grad = make_flat(grads) #grad = self.net.backward()\n",
        "        \n",
        "        # run CG\n",
        "        if self.init_delta is None:\n",
        "            self.init_delta = [torch.zeros_like(p, requires_grad=False) \n",
        "                               for p in self.params]\n",
        "        deltas = self.conjugate_gradient(self.init_delta, grads, iters=self.CG_iter)\n",
        "\n",
        "        self.init_delta = deltas[-1][1]  # note: don't backtrack this\n",
        "\n",
        "        # CG backtracking\n",
        "        new_err = np.inf\n",
        "        for j in range(len(deltas) - 1, -1, -1):\n",
        "            # new_params = []\n",
        "            # for group in self.param_groups:\n",
        "            #     for c, p in enumerate(group['params']):\n",
        "            #         new_params.append(p + deltas[j][1][c])\n",
        "            new_params = [self.params[p] + deltas[j][1][p] for p in range(0, len(self.params))]\n",
        "            prev_err = closure(new_params)\n",
        "            # prev_err = closure(W + deltas[j][1])\n",
        "            # print(\"prev\", prev_err, \"new\", new_err)\n",
        "            if prev_err > new_err:\n",
        "                break\n",
        "            delta = deltas[j][1]\n",
        "            new_err = prev_err\n",
        "        else:\n",
        "            j -= 1\n",
        "        \n",
        "        # update damping parameter (compare improvement predicted by\n",
        "        # quadratic model to the actual improvement in the error)\n",
        "        quad = (0.5 * inner_product(self.calc_G(delta, damping=self.damping), \n",
        "                                    delta) + inner_product(grads, delta))\n",
        "        # print(quad.item())\n",
        "        improvement_ratio = ((new_err - err) / quad) if quad != 0 else 1\n",
        "        if improvement_ratio < 0.25:\n",
        "            self.damping *= 1.5\n",
        "        elif improvement_ratio > 0.75:\n",
        "            self.damping *= 0.66\n",
        "        # print(improvement_ratio.item(), quad.item())\n",
        "        # line search to find learning rate\n",
        "        l_rate = 1.0\n",
        "        min_improv = min(1e-2 * inner_product(grads, delta), 0)\n",
        "        \n",
        "        for _ in range(60):\n",
        "            # check if the improvement is greater than the minimum\n",
        "            # improvement we would expect based on the starting gradient\n",
        "            if new_err <= err + l_rate * min_improv:\n",
        "                break\n",
        "            l_rate *= 0.8\n",
        "            new_params = [self.params[p] + (l_rate * delta[p]) for p in range(0, len(self.params))]\n",
        "            # new_params = []\n",
        "            # for group in self.param_groups:\n",
        "            #     for c, p in enumerate(group['params']):\n",
        "            #         new_params.append(p + l_rate * delta[c])\n",
        "            new_err = closure(new_params)\n",
        "            # print(\"prev_err\", err, \"new_err\", new_err)\n",
        "            # new_err = closure(W + l_rate * delta)\n",
        "        else:\n",
        "            # no good update, so skip this iteration\n",
        "            l_rate = 0.0\n",
        "            new_err = err\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for group in self.param_groups:\n",
        "                for c, p in enumerate(group['params']):\n",
        "                    p += l_rate * delta[c]\n",
        "                    if p.grad is not None:\n",
        "                        p.grad.detach_()\n",
        "                        p.grad.zero_()\n",
        "        # print(self.damping)\n",
        "        # assert False\n",
        "        return [l_rate * delta[c] for c in range(0, len(delta))]\n",
        "\n",
        "    def conjugate_gradient(self, init_delta, grad, iters=250):\n",
        "        \"\"\"Find minimum of quadratic approximation using conjugate gradient\n",
        "        algorithm.\"\"\"\n",
        "\n",
        "        store_iter = 5\n",
        "        store_mult = 1.3\n",
        "        deltas = []\n",
        "        vals = torch.zeros(iters) \n",
        "        base_grad = [g.mul(-1.0) for g in grad] # note negative, some CG algorithms are flipped\n",
        "        delta = [d.mul(0.95) for d in init_delta]\n",
        "        G_dir = [torch.zeros_like(g, requires_grad=False) for g in grad]\n",
        "        \n",
        "        residual = base_grad.copy()\n",
        "        self.calc_G(delta, damping=self.damping, out=G_dir)\n",
        "        for r in range(0, len(residual)): residual[r].sub_(G_dir[r])\n",
        "        res_norm = inner_product(residual, residual)\n",
        "        direction = residual.copy()\n",
        "        # print(direction)\n",
        "        \n",
        "        for i in range(iters):\n",
        "            # calculate step size\n",
        "            self.calc_G(direction, damping=self.damping, out=G_dir)\n",
        "            \n",
        "            step = res_norm / inner_product(direction, G_dir)\n",
        "            if ((step + 1) == step or (step != step)):\n",
        "                print(\"Non-finite step value (%f)\" % step)\n",
        "                break\n",
        "\n",
        "            # update weight delta\n",
        "            for d in range(0, len(delta)): delta[d] += step * direction[d]\n",
        "            \n",
        "            # update residual\n",
        "            for r in range(0, len(residual)): residual[r].sub_(step * G_dir[r])\n",
        "            new_res_norm = inner_product(residual, residual)\n",
        "\n",
        "            if new_res_norm < 1e-20:\n",
        "                # early termination (mainly to prevent numerical errors);\n",
        "                # the main termination condition is below.\n",
        "                break\n",
        "\n",
        "            # update direction\n",
        "            beta = new_res_norm / res_norm\n",
        "            for d in range(0, len(direction)): \n",
        "                direction[d] = beta * direction[d] + residual[d]\n",
        "            # direction *= beta\n",
        "            # direction += residual\n",
        "\n",
        "            res_norm = new_res_norm\n",
        "\n",
        "            # store deltas for backtracking\n",
        "            if i == store_iter:\n",
        "                deltas += [(i, delta.copy())]\n",
        "                store_iter = int(store_iter * store_mult)\n",
        "\n",
        "            # martens termination conditions\n",
        "            temp = [residual[r] + base_grad[r] for r in range(0, len(residual))]\n",
        "            vals[i] = -0.5 * inner_product(temp, delta)\n",
        "            \n",
        "            gap = max(int(0.1 * i), 10)\n",
        "\n",
        "            if (i > gap and vals[i - gap] < 0 \n",
        "                    and (vals[i] - vals[i - gap]) / vals[i] < 5e-6 * gap):\n",
        "                break\n",
        "\n",
        "        deltas += [(i, delta.copy())]\n",
        "        return deltas\n",
        "\n",
        "    def calc_G(self, v, damping=0, out=None):\n",
        "        \"\"\"Compute Gauss-Newton matrix-vector product.\"\"\"\n",
        "\n",
        "        if out is None:\n",
        "            Gv = [torch.zeros_like(val, requires_grad=False) for val in v]\n",
        "        else:\n",
        "            Gv = out\n",
        "            for val in Gv: val.fill_(0.0)\n",
        "\n",
        "        # R forward pass\n",
        "        R_activations = [torch.zeros_like(a, requires_grad=False) for a in self.net.activations] \n",
        "        for i in range(1, len(R_activations)):\n",
        "            R_activations[i].add_(self.net.activations[(i-1)] @ v[(i-1)*2].t() + v[(i-1)*2 + 1].unsqueeze(0))\n",
        "            R_activations[i].add_(R_activations[(i-1)] @ self.params[(i-1)*2].t())\n",
        "            R_activations[i].mul_(self.net.d_activations[i])\n",
        "            # R_activations[i] = J_dot(self.net.d_activations[i], R_activations[i])\n",
        "        \n",
        "        # backward pass\n",
        "        R_error = R_activations\n",
        "        for i in range(len(R_error)-1, 0, -1):\n",
        "            if (i == len(R_error)-1): \n",
        "                R_error[i] *= self.net.d2_loss_v\n",
        "            else:\n",
        "                R_error[i].fill_(0.0)\n",
        "\n",
        "            # RDs[i] = RDa[i] * self.net.d_activations[i].t() \n",
        "            # RDW[i] = RDs[i] @ self.net.activations[(i-1)].t()\n",
        "            # RDb[i] = RDs[i]\n",
        "            # RDa[i-1] = params[i*2].t() @ RDs[i]\n",
        "\n",
        "            R_error[i] *= self.net.d_activations[i]\n",
        "            Gv[(i-1)*2] = R_error[i].t() @ self.net.activations[i-1]\n",
        "            Gv[(i-1)*2+1] = torch.sum(R_error[i], 0)\n",
        "            R_error[i-1] += R_error[i] @ self.params[(i-1)*2]\n",
        "        # print(R_activations[0].shape[0])\n",
        "        for val in Gv: val /= (R_activations[0].shape[0])\n",
        "\n",
        "        for i in range(0, len(Gv)): Gv[i] += damping * v[i]  # Tikhonov damping\n",
        "        return Gv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKjyLqtcd9pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    def __init__(self, *layers, shapes=None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.act_means = [[] for _ in layers]\n",
        "        self.act_stds  = [[] for _ in layers]\n",
        "        self.d2_loss_v = None\n",
        "        self.shapes = shapes\n",
        "        self.compute_offsets()\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        self.activations = []\n",
        "        self.d_activations = []\n",
        "        self.activations.append(x)\n",
        "        self.d_activations.append(x == x)\n",
        "        for i,l in enumerate(self.layers):\n",
        "            out_x = l(x)\n",
        "            if (isinstance(l, type(nn.ReLU()))):\n",
        "                self.activations.append(x)\n",
        "                self.d_activations.append(x == out_x)\n",
        "            x = out_x\n",
        "            self.act_means[i].append(x.data.mean())\n",
        "            self.act_stds [i].append(x.data.std())\n",
        "        return x\n",
        "    \n",
        "    def __iter__(self): return iter(self.layers)\n",
        "\n",
        "    def compute_offsets(self):\n",
        "        \"\"\"Precompute offsets for layers in the overall parameter vector.\"\"\"\n",
        "        self.offsets = {}\n",
        "        offset = 0\n",
        "        for i in range(0, len(self.shapes)):\n",
        "            n_params = (self.shapes[i][0] + 1) * self.shapes[i][1]\n",
        "            self.offsets[i] = (offset, \n",
        "                               offset + n_params - self.shapes[i][1], \n",
        "                               offset + n_params)\n",
        "            offset += n_params\n",
        "        return offset\n",
        "\n",
        "    def get_weights(self, params, conn):\n",
        "        \"\"\"Get weight matrix for a connection from overall parameter vector.\"\"\"\n",
        "\n",
        "        if conn not in self.offsets:\n",
        "            return None\n",
        "\n",
        "        offset, W_end, b_end = self.offsets[conn]\n",
        "        W = params[offset:W_end]\n",
        "        b = params[W_end:b_end]\n",
        "        return W.reshape((self.shape[conn[0]], self.shape[conn[1]])), b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCORohtNd9sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(data, lr=0.5, nh=50):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    shapes = [(m, nh), (nh, data.c)]\n",
        "    model = SequentialModel(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c), nn.ReLU(), shapes=shapes) \n",
        "    return model, HessianFree(model.parameters(), CG_iter=500, init_damping=20, net=model) #optim.SGD(model.parameters(), lr=lr) \n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJQ-UCk5d9yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_learn = Learner(*get_model(toy_data), loss_func, toy_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zymzdNtqd9wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(*get_model(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIFi48Pmd9mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, learn):\n",
        "    for epoch in range(epochs):\n",
        "        learn.model.train()\n",
        "        for xb,yb in learn.data.train_dl:\n",
        "            y_hat = learn.model(xb)\n",
        "            loss = learn.loss_func(y_hat, yb)\n",
        "            learn.model.d2_loss_v = d2_loss(y_hat, yb)\n",
        "            loss.backward()\n",
        "            # print(get_loss(y_hat, yb))\n",
        "            def closure(new_params=None):\n",
        "                if new_params:\n",
        "                    y_hat_new = xb.clone().detach()\n",
        "                    for i in range(0, len(new_params), 2):\n",
        "                        y_hat_new = y_hat_new @ new_params[i].t() + new_params[i+1]\n",
        "                        y_hat_new.clamp_min_(0.)\n",
        "                    loss = learn.loss_func(y_hat_new, yb)\n",
        "                    # loss = get_loss(y_hat_new, yb)\n",
        "                    # print(loss)\n",
        "                else:\n",
        "                    loss = learn.loss_func(learn.model(xb), yb)\n",
        "                    # loss = get_loss(learn.model(xb), yb)\n",
        "                return loss\n",
        "            update = learn.opt.step(closure)\n",
        "            learn.opt.zero_grad()\n",
        "            # print(update)\n",
        "            # with torch.no_grad():\n",
        "            #     for c, p in enumerate(learn.model.parameters()):\n",
        "            #         p += update[c]\n",
        "            # with torch.no_grad():\n",
        "            #     for c, p in enumerate(learn.model.parameters()):\n",
        "            #         print(p)\n",
        "        learn.model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc = 0.0, 0.0\n",
        "            for xb,yb in learn.data.valid_dl:\n",
        "                pred = learn.model(xb)\n",
        "                tot_loss += learn.loss_func(pred, yb)\n",
        "                tot_acc  += accuracy (pred,yb)\n",
        "        nv = len(learn.data.valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "        torch.cuda.empty_cache()\n",
        "    return tot_loss/nv, tot_acc/nv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgKxSfAgd9eT",
        "colab_type": "code",
        "outputId": "6ddd78ef-e9f8-4632-95b4-d44dde0e2315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "def make_flat_v2(tensor_list):\n",
        "    temp = []\n",
        "    for c in range(0, len(tensor_list)):\n",
        "        temp.append(tensor_list[c].view(-1))\n",
        "    return torch.cat(temp, dim=0)\n",
        "\n",
        "def inner_product(tens1, tens2):\n",
        "    if len(tens1) != len(tens2): return None\n",
        "    product = 0\n",
        "    for i in range(0, len(tens1)):\n",
        "        product += torch.matmul(tens1[i].view(-1), tens2[i].view(-1))\n",
        "    return product\n",
        "\n",
        "start = [0, 8, 12, 20, 22]\n",
        "end = [8, 12, 20, 22, 24]\n",
        "def get(array, i, shape):\n",
        "    s = start[i] \n",
        "    e = end[i]\n",
        "    return array[s:e].view(shape)\n",
        "\n",
        "m = torch.tensor(np.arange(22))\n",
        "print(get(m, 0, (2,4)))\n",
        "x = torch.tensor([[1,5], [2,6], [3, 7], [4, 8]])\n",
        "y = torch.tensor([17, 18, 19, 20])\n",
        "z = torch.tensor([[9, 11, 13,15], [10, 12, 14, 16]])\n",
        "w = torch.tensor([21, 22])\n",
        "ar = [x, y, z, w]\n",
        "shapes = []\n",
        "for s in ar: \n",
        "    print(s.view(-1))\n",
        "    shapes.append(s.shape)\n",
        "    # print(s.t().shape)\n",
        "\n",
        "f = make_flat_v2(ar)\n",
        "# for i,d in enumerate(f):\n",
        "#     d.reshape(shapes[i])\n",
        "print(f)\n",
        "print(make_flat(x), make_flat(z))\n",
        "# quad = (0.5 * torch.matmul(make_flat(temp_G), flat_delta) + torch.matmul(grad, flat_delta))\n",
        "torch.matmul(make_flat(x), make_flat(z.t())), inner_product([x], [z])\n",
        "# (make_flat(x) * make_flat(z).t()).sum()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 1, 2, 3],\n",
            "        [4, 5, 6, 7]])\n",
            "tensor([1, 5, 2, 6, 3, 7, 4, 8])\n",
            "tensor([17, 18, 19, 20])\n",
            "tensor([ 9, 11, 13, 15, 10, 12, 14, 16])\n",
            "tensor([21, 22])\n",
            "tensor([ 1,  5,  2,  6,  3,  7,  4,  8, 17, 18, 19, 20,  9, 11, 13, 15, 10, 12,\n",
            "        14, 16, 21, 22])\n",
            "tensor([1, 5, 2, 6, 3, 7, 4, 8]) tensor([ 9, 11, 13, 15, 10, 12, 14, 16])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(478), tensor(478))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3jEYYY5bUqV",
        "colab_type": "code",
        "outputId": "30d8e41c-cbca-4a82-dcff-31fe96b796fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# torch.diag_embed(output, offset=0, dim1=-2, dim2=-1) - output.unsqueeze(-1) @ output.unsqueeze(1)\n",
        "y = torch.tensor([[0], [2], [2]])\n",
        "y_new = [[0, 0, 0, 0] for temp in y]\n",
        "for i in range(0, len(y_new)): y_new[i][y[i]] = 1\n",
        "y_new = torch.tensor(y_new)\n",
        "x = torch.tensor([[1., 2., 3., 4.], [2, 3, 4, 5], [3, 4, 5, 6]])\n",
        "bd2 = torch.diag_embed(x, offset=0, dim1=-2, dim2=-1) - x.unsqueeze(-1) @ x.unsqueeze(1)\n",
        "d1 = torch.diag(x[0]) - x[0].unsqueeze(-1).t() @ x[0].unsqueeze(1)\n",
        "print(y_new)\n",
        "print(x.shape, y.shape)\n",
        "print(y_new / (x ** 2))\n",
        "y_new = torch.zeros_like(x)\n",
        "for i in range(0, len(y_new)): y_new[i][y[i]] = 1\n",
        "print(y_new)\n",
        "print(y_new / (x ** 2))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 0, 0],\n",
            "        [0, 0, 1, 0],\n",
            "        [0, 0, 1, 0]])\n",
            "torch.Size([3, 4]) torch.Size([3, 1])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0625, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0400, 0.0000]])\n",
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0625, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0400, 0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGIPLP0leevb",
        "colab_type": "code",
        "outputId": "d13e9980-51dd-427d-d34d-26a391299db4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "fit(50, toy_learn)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.6986) tensor(0.5000)\n",
            "1 tensor(0.6885) tensor(0.5000)\n",
            "2 tensor(0.6801) tensor(0.5000)\n",
            "3 tensor(0.6724) tensor(0.7500)\n",
            "4 tensor(0.6622) tensor(1.)\n",
            "5 tensor(0.6469) tensor(1.)\n",
            "6 tensor(0.6238) tensor(1.)\n",
            "7 tensor(0.5905) tensor(1.)\n",
            "8 tensor(0.5380) tensor(1.)\n",
            "9 tensor(0.4791) tensor(1.)\n",
            "10 tensor(0.4575) tensor(0.7500)\n",
            "11 tensor(0.4355) tensor(1.)\n",
            "12 tensor(0.4205) tensor(0.7500)\n",
            "13 tensor(0.4200) tensor(0.7500)\n",
            "14 tensor(0.3951) tensor(1.)\n",
            "15 tensor(0.3908) tensor(1.)\n",
            "16 tensor(0.3905) tensor(1.)\n",
            "17 tensor(0.3892) tensor(1.)\n",
            "18 tensor(0.3892) tensor(1.)\n",
            "19 tensor(0.3811) tensor(1.)\n",
            "20 tensor(0.3715) tensor(1.)\n",
            "21 tensor(0.3650) tensor(1.)\n",
            "22 tensor(0.3599) tensor(1.)\n",
            "23 tensor(0.3595) tensor(1.)\n",
            "24 tensor(0.3595) tensor(1.)\n",
            "25 tensor(0.3575) tensor(1.)\n",
            "26 tensor(0.3511) tensor(1.)\n",
            "27 tensor(0.3405) tensor(1.)\n",
            "28 tensor(0.3097) tensor(1.)\n",
            "29 tensor(0.3077) tensor(1.)\n",
            "30 tensor(0.3077) tensor(1.)\n",
            "31 tensor(0.3077) tensor(1.)\n",
            "32 tensor(0.3077) tensor(1.)\n",
            "33 tensor(0.3054) tensor(1.)\n",
            "34 tensor(0.3054) tensor(1.)\n",
            "35 tensor(0.3054) tensor(1.)\n",
            "36 tensor(0.3037) tensor(1.)\n",
            "37 tensor(0.3010) tensor(1.)\n",
            "38 tensor(0.2970) tensor(1.)\n",
            "39 tensor(0.2913) tensor(1.)\n",
            "40 tensor(0.2829) tensor(1.)\n",
            "41 tensor(0.2801) tensor(1.)\n",
            "42 tensor(0.2710) tensor(1.)\n",
            "43 tensor(0.2561) tensor(1.)\n",
            "44 tensor(0.2516) tensor(1.)\n",
            "45 tensor(0.2434) tensor(1.)\n",
            "46 tensor(0.2411) tensor(1.)\n",
            "47 tensor(0.2411) tensor(1.)\n",
            "48 tensor(0.2403) tensor(1.)\n",
            "49 tensor(0.2403) tensor(1.)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.2403), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP6rFxG_eezf",
        "colab_type": "code",
        "outputId": "afde20d1-c184-45a9-dcd6-aae70541c278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "fit(5, learn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(1.2801) tensor(0.4077)\n",
            "1 tensor(1.0982) tensor(0.3268)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWWC4EPDee7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "924d213f-7852-4e46-c312-0cea56406159"
      },
      "source": [
        "ar1 = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "ar2 = np.array([[1,0], [0,1], [0,1], [1,0]])\n",
        "pr = np.array([[[0.2,0.7], [0.2,0.1], [0.8,0.9], [0.8,0.5]],\n",
        "               [[0.3,0.7], [0.2,0.1], [0.8,0.9], [0.8,0.5]],\n",
        "               [[0.2,0.7], [0.2,0.1], [0.8,0.9], [0.8,0.5]]])\n",
        "losses = np.sum(np.nan_to_num(pr - ar2) ** 2, axis=tuple(range(1, pr.ndim))) / 2\n",
        "losses = -np.sum(np.nan_to_num(ar2) * np.log(pr), axis=tuple(range(1, pr.ndim)))\n",
        "print(losses)\n",
        "check_loss = np.sum([np.true_divide(np.sum(l), 1) for l in losses if l is not None])\n",
        "print(check_loss)\n",
        "new_pr = (torch.tensor([t.flatten() for t in pr]))\n",
        "print(new_pr.shape)\n",
        "print(torch.diag(torch.tensor(pr[0].flatten()) * (1 - pr[0].flatten())))\n",
        "assert (new_pr.unsqueeze(-1) @ new_pr.unsqueeze(1)).shape == torch.diag_embed(new_pr, offset=0, dim1=-2, dim2=-1).shape\n",
        "testing = torch.sum(torch.diag_embed(new_pr, offset=0, dim1=-2, dim2=-1) - new_pr.unsqueeze(-1) @ new_pr.unsqueeze(1), 0)\n",
        "print(testing.clamp_min(0.0))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.240527 3.835062 4.240527]\n",
            "12.31611610909238\n",
            "torch.Size([3, 8])\n",
            "tensor([[0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.2100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.1600, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0900, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0900, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1600, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2500]],\n",
            "       dtype=torch.float64)\n",
            "tensor([[0.5300, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.6300, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.4800, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.2700, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.4800, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2700, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4800, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7500]],\n",
            "       dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYGTPSNlJdyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}