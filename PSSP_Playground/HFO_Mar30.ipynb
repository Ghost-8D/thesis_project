{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HFO_Mar30.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLNr9VVSbCQ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ce210ec-0a31-4f35-9427-6cc3d2346c88"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r759K-obiQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "import operator\n",
        "\n",
        "def test(a,b,cmp,cname=None):\n",
        "    if cname is None: cname=cmp.__name__\n",
        "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
        "\n",
        "def test_eq(a,b): test(a,b,operator.eq,'==')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MP9le5mbiTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#export\n",
        "from pathlib import Path\n",
        "from IPython.core.debugger import set_trace\n",
        "from fastai import datasets\n",
        "import pickle, gzip, math, torch, matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import tensor\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXteObIKbiWb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plus 10 data\n",
        "train_url = 'https://raw.githubusercontent.com/Ghost-8D/thesis_project/master/PSSP_Playground/datasetsPlus10/trainSet0.txt'\n",
        "test_url = 'https://raw.githubusercontent.com/Ghost-8D/thesis_project/master/PSSP_Playground/datasetsPlus10/testSet0.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub_JzN6ybiZV",
        "colab_type": "code",
        "outputId": "e7db99f5-2c67-4a44-88cb-efda0b0da76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! mkdir pssp_model\n",
        "! ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pssp_model  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOK_s2LYbicZ",
        "colab_type": "code",
        "outputId": "be6d0882-42ae-46cf-b891-58c9a096442e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = Path('/pssp_model')\n",
        "path.mkdir(parents=True, exist_ok=True)\n",
        "path"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/pssp_model')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhsp8wajbifJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    df_train = pd.read_csv(train_url, header=None)\n",
        "    df_valid = pd.read_csv(test_url, header=None)\n",
        "    def get_XY_from_df(df):\n",
        "        x = df.values[:, :-1]\n",
        "        y = df.values[:, TOTAL_COLS-1]\n",
        "        return x, y\n",
        "    x_train, y_train = get_XY_from_df(df_train)\n",
        "    x_valid, y_valid = get_XY_from_df(df_valid)\n",
        "    x_train, y_train, x_valid, y_valid = map(tensor, (x_train, y_train, x_valid, y_valid))\n",
        "    return x_train.float(), y_train, x_valid.float(), y_valid\n",
        "\n",
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfCiaGQ0bih2",
        "colab_type": "code",
        "outputId": "a4a90c7d-d5d8-452b-c382-6710bbcf448f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "NEIGHBOURS = 10 # 1 amino-acid left and 1 amino-acid right added \n",
        "AMINO_ACID_LEN = 20\n",
        "WINDOW = 2 * NEIGHBOURS + 1\n",
        "TOTAL_AMINO_ACIDS = WINDOW * AMINO_ACID_LEN\n",
        "TOTAL_COLS = TOTAL_AMINO_ACIDS + 1 # plus the secondary structure category\n",
        "CATEGORIES = 3\n",
        "TOTAL_COLS"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "421"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYF7Ax5Bbikl",
        "colab_type": "code",
        "outputId": "a4bc97a9-158f-4f21-a6e8-78d404374258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "x_train, y_train, x_valid, y_valid = get_data()\n",
        "n,c = x_train.shape\n",
        "x_train, x_train.shape, y_train, y_train.shape, y_train.min(), y_train.max()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         ...,\n",
              "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
              "         [18., 10., 64.,  ...,  0.,  0.,  0.]]),\n",
              " torch.Size([77092, 420]),\n",
              " tensor([0, 0, 1,  ..., 1, 0, 0]),\n",
              " torch.Size([77092]),\n",
              " tensor(0),\n",
              " tensor(2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuT8suKKbinc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert n==y_train.shape[0]==77092\n",
        "test_eq(c, WINDOW * AMINO_ACID_LEN)\n",
        "test_eq(y_train.min(),0)\n",
        "test_eq(y_train.max(),2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o7nK_YfbiqQ",
        "colab_type": "code",
        "outputId": "99a46d65-6c0a-4561-929c-df42114d8ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([77092, 420]),\n",
              " torch.Size([77092]),\n",
              " torch.Size([7289, 420]),\n",
              " torch.Size([7289]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpbhdFK_bitG",
        "colab_type": "code",
        "outputId": "c304ad73-4ba9-4948-842c-567fe66130b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mpl.rcParams['image.cmap'] = 'cool'\n",
        "img = x_train[0]\n",
        "img.view(WINDOW, AMINO_ACID_LEN).type()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XQTu4HIbiv4",
        "colab_type": "code",
        "outputId": "ed7cd546-7c50-4ff2-aa35-c14b5ee0a184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.imshow(img.view((21,20)));"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n05O0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJAWVlPibiyy",
        "colab_type": "code",
        "outputId": "a821cf74-939b-4936-f5c5-b43d106ef8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_mean, train_std = x_train.mean(), x_train.std()\n",
        "train_mean, train_std"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(4.7987), tensor(11.2503))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwDtY1c0cE6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x, m, s): return (x-m)/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT2I4GQGcE-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = normalize(x_train, train_mean, train_std)\n",
        "# NOTE: Use training, not validation mean for validation set\n",
        "x_valid = normalize(x_valid, train_mean, train_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbyYxfMicFCC",
        "colab_type": "code",
        "outputId": "dfde4eca-6661-4f17-c037-bd03722a383a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_mean, train_std = x_train.mean(), x_train.std()\n",
        "train_mean, train_std"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-4.1988e-05), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1veWQtEcFFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuxMCHiqcFIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Module():\n",
        "    def __call__(self, *args):\n",
        "        self.args = args\n",
        "        self.out = self.forward(*args)\n",
        "        return self.out\n",
        "    \n",
        "    def forward(self): raise Exception('not implemented')\n",
        "    def backward(self): self.bwd(self.out, *self.args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReqApxWacFLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lin(Module):\n",
        "    def __init__(self, w, b): \n",
        "        super().__init__()\n",
        "        self.w,self.b = w,b\n",
        "    \n",
        "    def forward(self, inp): return inp@self.w + self.b\n",
        "    \n",
        "    def bwd(self, out, inp):\n",
        "        inp.g = out.g @ self.w.t()\n",
        "        self.w.g = inp.t() @ out.g\n",
        "        self.b.g = out.g.sum(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PleUZK1ocFON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Relu(Module):\n",
        "    def __init__(self): super().__init__()\n",
        "    def forward(self, inp): return torch.clamp(inp, 0, 1e10) #return inp.clamp_min(0.)-0.5\n",
        "    def bwd(self, out, inp): return out == inp #inp.g = (inp>0).float() * out.g"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2taNtADcFRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model():\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        self.layers = nn.ModuleList([Lin(n_in, nh), Relu(), Lin(nh, n_out)])\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def __call__(self, x, targ):\n",
        "        for l in self.layers: x = l(x)\n",
        "        return self.loss(x, targ)\n",
        "    \n",
        "    def backward(self):\n",
        "        self.loss.backward()\n",
        "        for l in reversed(self.layers): l.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV2bNXsLcFUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, n_in, nh, n_out):\n",
        "        super().__init__()\n",
        "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out), nn.ReLU()]\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "        self.params = [torch.randn(n_in, nh) * math.sqrt(2.0/n_in),\n",
        "                           torch.randn(nh),\n",
        "                           torch.randn(nh, n_out) * math.sqrt(2.0/nh),\n",
        "                           torch.randn(n_out)]\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        self.activations = []\n",
        "        self.d_activations = []\n",
        "        for l in self.layers: \n",
        "            out_x = l(x)\n",
        "            if (isinstance(l, type(nn.ReLU()))):\n",
        "                self.activations.append(x)\n",
        "                self.d_activations.append(x == out_x)\n",
        "            x = out_x\n",
        "        return x \n",
        "\n",
        "    def parameters(self):\n",
        "        return self.params"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1aHOmplcdFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXu926bTcdIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Optimizer():\n",
        "    def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr\n",
        "        \n",
        "    def step(self):\n",
        "        with torch.no_grad():\n",
        "            for p in self.params: p -= p.grad * lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        for p in self.params: p.grad.data.zero_()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLeBnnUYcdLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self, x, y): self.x,self.y = x,y\n",
        "    def __len__(self): return len(self.x)\n",
        "    def __getitem__(self, i): return self.x[i],self.y[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSfAqeDcdOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
        "def get_dls(train_ds, valid_ds, bs, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdBXMkIvcdRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, c=None):\n",
        "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
        "        \n",
        "    @property\n",
        "    def train_ds(self): return self.train_dl.dataset\n",
        "        \n",
        "    @property\n",
        "    def valid_ds(self): return self.valid_dl.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA32Xh0VcdUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train,y_train,x_valid,y_valid = get_data()\n",
        "train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)\n",
        "assert len(train_ds)==len(x_train)\n",
        "assert len(valid_ds)==len(x_valid)\n",
        "nh,bs = 50, 78000\n",
        "c = y_train.max().item()+1\n",
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJuodP7tcdXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWM7tz1ycufh",
        "colab_type": "code",
        "outputId": "914dcd64-598a-48b5-870a-d393f5cbff91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = nn.Sequential(nn.Linear(420,50), nn.ReLU(), nn.Linear(50,3))\n",
        "model.parameters()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7f28fc4deeb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-409B9TCcuiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(data, lr=0.5, nh=50):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c)) #Model(m, nh, data.c) \n",
        "    return model, optim.SGD(model.parameters(), lr=lr) #HessianFree(model.parameters(), CG_iter=100, init_damping=20, net=model)\n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A33rrv0pcumE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(*get_model(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSRr7Q7acupj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, learn):\n",
        "    for epoch in range(epochs):\n",
        "        learn.model.train()\n",
        "        for xb,yb in learn.data.train_dl:\n",
        "            loss = learn.loss_func(learn.model(xb), yb)\n",
        "            loss.backward()\n",
        "            learn.opt.step()\n",
        "            learn.opt.zero_grad()\n",
        "\n",
        "        learn.model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc = 0.0, 0.0\n",
        "            for xb,yb in learn.data.valid_dl:\n",
        "                pred = learn.model(xb)\n",
        "                tot_loss += learn.loss_func(pred, yb)\n",
        "                tot_acc  += accuracy (pred,yb)\n",
        "        nv = len(learn.data.valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "    return tot_loss/nv, tot_acc/nv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjboABr3cuso",
        "colab_type": "code",
        "outputId": "e5cc6a06-fb15-4f81-89d8-342442597447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "loss,acc = fit(10, learn)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(62.5205) tensor(0.2456)\n",
            "1 tensor(3.6233) tensor(0.4752)\n",
            "2 tensor(1.1012) tensor(0.3315)\n",
            "3 tensor(1.0922) tensor(0.3349)\n",
            "4 tensor(1.0859) tensor(0.3383)\n",
            "5 tensor(1.0811) tensor(0.3411)\n",
            "6 tensor(1.0775) tensor(0.3489)\n",
            "7 tensor(1.0744) tensor(0.4276)\n",
            "8 tensor(1.0709) tensor(0.4276)\n",
            "9 tensor(1.0649) tensor(0.4276)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl_r6flpcuwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "\n",
        "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
        "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
        "def camel2snake(name):\n",
        "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
        "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()\n",
        "\n",
        "class Callback():\n",
        "    _order=0\n",
        "    def set_runner(self, run): self.run=run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "    @property\n",
        "    def name(self):\n",
        "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
        "        return camel2snake(name or 'callback')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Szwr-biXcdbI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TrainEvalCallback(Callback):\n",
        "    def begin_fit(self):\n",
        "        self.run.n_epochs=0.\n",
        "        self.run.n_iter=0\n",
        "    \n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.run.n_epochs += 1./self.iters\n",
        "        self.run.n_iter   += 1\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.run.n_epochs=self.epoch\n",
        "        self.model.train()\n",
        "        self.run.in_train=True\n",
        "\n",
        "    def begin_validate(self):\n",
        "        self.model.eval()\n",
        "        self.run.in_train=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stz48NH9dEoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import *\n",
        "from functools import partial\n",
        "\n",
        "def listify(o):\n",
        "    if o is None: return []\n",
        "    if isinstance(o, list): return o\n",
        "    if isinstance(o, str): return [o]\n",
        "    if isinstance(o, Iterable): return list(o)\n",
        "    return [o]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l9D6BovdEsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Runner():\n",
        "    def __init__(self, cbs=None, cb_funcs=None):\n",
        "        cbs = listify(cbs)\n",
        "        for cbf in listify(cb_funcs):\n",
        "            cb = cbf()\n",
        "            setattr(self, cb.name, cb)\n",
        "            cbs.append(cb)\n",
        "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
        "\n",
        "    @property\n",
        "    def opt(self):       return self.learn.opt\n",
        "    @property\n",
        "    def model(self):     return self.learn.model\n",
        "    @property\n",
        "    def loss_func(self): return self.learn.loss_func\n",
        "    @property\n",
        "    def data(self):      return self.learn.data\n",
        "\n",
        "    def one_batch(self, xb, yb):\n",
        "        self.xb,self.yb = xb,yb\n",
        "        if self('begin_batch'): return\n",
        "        self.pred = self.model(self.xb)\n",
        "        if self('after_pred'): return\n",
        "        self.loss = self.loss_func(self.pred, self.yb)\n",
        "        if self('after_loss') or not self.in_train: return\n",
        "        self.loss.backward()\n",
        "        if self('after_backward'): return\n",
        "        self.opt.step()\n",
        "        if self('after_step'): return\n",
        "        self.opt.zero_grad()\n",
        "\n",
        "    def all_batches(self, dl):\n",
        "        self.iters = len(dl)\n",
        "        for xb,yb in dl:\n",
        "            if self.stop: break\n",
        "            self.one_batch(xb, yb)\n",
        "            self('after_batch')\n",
        "        self.stop=False\n",
        "\n",
        "    def fit(self, epochs, learn):\n",
        "        self.epochs,self.learn = epochs,learn\n",
        "\n",
        "        try:\n",
        "            for cb in self.cbs: cb.set_runner(self)\n",
        "            if self('begin_fit'): return\n",
        "            for epoch in range(epochs):\n",
        "                self.epoch = epoch\n",
        "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "\n",
        "                with torch.no_grad(): \n",
        "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "                if self('after_epoch'): break\n",
        "            \n",
        "        finally:\n",
        "            self('after_fit')\n",
        "            self.learn = None\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
        "            f = getattr(cb, cb_name, None)\n",
        "            if f and f(): return True\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74UDlpVcdEvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AvgStats():\n",
        "    def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train\n",
        "    \n",
        "    def reset(self):\n",
        "        self.tot_loss,self.count = 0.,0\n",
        "        self.tot_mets = [0.] * len(self.metrics)\n",
        "        \n",
        "    @property\n",
        "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
        "    @property\n",
        "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
        "    \n",
        "    def __repr__(self):\n",
        "        if not self.count: return \"\"\n",
        "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
        "\n",
        "    def accumulate(self, run):\n",
        "        bn = run.xb.shape[0]\n",
        "        self.tot_loss += run.loss * bn\n",
        "        self.count += bn\n",
        "        for i,m in enumerate(self.metrics):\n",
        "            self.tot_mets[i] += m(run.pred, run.yb) * bn\n",
        "\n",
        "class AvgStatsCallback(Callback):\n",
        "    def __init__(self, metrics):\n",
        "        self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)\n",
        "        \n",
        "    def begin_epoch(self):\n",
        "        self.train_stats.reset()\n",
        "        self.valid_stats.reset()\n",
        "        \n",
        "    def after_loss(self):\n",
        "        stats = self.train_stats if self.in_train else self.valid_stats\n",
        "        with torch.no_grad(): stats.accumulate(self.run)\n",
        "    \n",
        "    def after_epoch(self):\n",
        "        print(self.train_stats)\n",
        "        print(self.valid_stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxB1QGtOdEyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_learner(model_func, loss_func, data):\n",
        "    return Learner(*model_func(data), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpJP_1nndE1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model_func(lr=0.5): return partial(get_model, lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bacyws7udSsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Recorder(Callback):\n",
        "    def begin_fit(self): self.lrs,self.losses = [],[]\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.lrs.append(self.opt.param_groups[-1]['lr'])\n",
        "        self.losses.append(self.loss.detach().cpu())        \n",
        "\n",
        "    def plot_lr  (self): plt.plot(self.lrs)\n",
        "    def plot_loss(self): plt.plot(self.losses)\n",
        "\n",
        "class ParamScheduler(Callback):\n",
        "    _order=1\n",
        "    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func\n",
        "\n",
        "    def set_param(self):\n",
        "        for pg in self.opt.param_groups:\n",
        "            pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)\n",
        "            \n",
        "    def begin_batch(self): \n",
        "        if self.in_train: self.set_param()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmwd8ke0dSvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def annealer(f):\n",
        "    def _inner(start, end): return partial(f, start, end)\n",
        "    return _inner\n",
        "\n",
        "@annealer\n",
        "def sched_lin(start, end, pos): return start + pos*(end-start)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DDCWRNHdSyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@annealer\n",
        "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
        "@annealer\n",
        "def sched_no(start, end, pos):  return start\n",
        "@annealer\n",
        "def sched_exp(start, end, pos): return start * (end/start) ** pos\n",
        "\n",
        "def cos_1cycle_anneal(start, high, end):\n",
        "    return [sched_cos(start, high), sched_cos(high, end)]\n",
        "\n",
        "#This monkey-patch is there to be able to plot tensors\n",
        "torch.Tensor.ndim = property(lambda x: len(x.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM-EMA9sdS1n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_scheds(pcts, scheds):\n",
        "    assert sum(pcts) == 1.\n",
        "    pcts = tensor([0] + listify(pcts))\n",
        "    assert torch.all(pcts >= 0)\n",
        "    pcts = torch.cumsum(pcts, 0)\n",
        "    def _inner(pos):\n",
        "        idx = (pos >= pcts).nonzero().max()\n",
        "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
        "        return scheds[idx](actual_pos)\n",
        "    return _inner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBdiKCUIdS5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_to(train, valid):\n",
        "    m,s = train.mean(),train.std()\n",
        "    return normalize(train, m, s), normalize(valid, m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1W27uIAdgRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lambda(nn.Module):\n",
        "    def __init__(self, func):\n",
        "        super().__init__()\n",
        "        self.func = func\n",
        "\n",
        "    def forward(self, x): return self.func(x)\n",
        "\n",
        "def flatten(x): return x.view(x.shape[0], -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sowOzwbPdgUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cnn_model(data):\n",
        "    return nn.Sequential(\n",
        "        Lambda(pssp_resize),\n",
        "        nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14\n",
        "        nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7\n",
        "        nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4\n",
        "        nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2\n",
        "        nn.AdaptiveAvgPool2d(1),\n",
        "        Lambda(flatten),\n",
        "        nn.Linear(32,data.c)\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57kvTSQldga6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CudaCallback(Callback):\n",
        "    def begin_fit(self): self.model.cuda()\n",
        "    def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwIuvXZ1dggz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchTransformXCallback(Callback):\n",
        "    _order=2\n",
        "    def __init__(self, tfm): self.tfm = tfm\n",
        "    def begin_batch(self): self.run.xb = self.tfm(self.xb)\n",
        "\n",
        "def view_tfm(*size):\n",
        "    def _inner(x): return x.view(*((-1,)+size))\n",
        "    return _inner"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yuz6sOwdgkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cnn_layers(data, nfs):\n",
        "    nfs = [1] + nfs\n",
        "    return [\n",
        "        conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3)\n",
        "        for i in range(len(nfs)-1)\n",
        "    ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]\n",
        "\n",
        "def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2kRzN8Ddgov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(ni, nf, ks=3, stride=2):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8buGHaUIdgs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):\n",
        "    if opt_func is None: opt_func = optim.SGD\n",
        "    opt = opt_func(model.parameters(), lr=lr)\n",
        "    learn = Learner(model, opt, loss_func, data)\n",
        "    return learn, Runner(cb_funcs=listify(cbs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoW_NR-IdgfL",
        "colab_type": "code",
        "outputId": "293ced08-10d9-4b76-a36a-3daab02a12b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "cbfs = [Recorder, partial(AvgStatsCallback,accuracy)]\n",
        "cbfs.append(CudaCallback)\n",
        "pssp_view = view_tfm(1,21,20)\n",
        "cbfs.append(partial(BatchTransformXCallback, pssp_view))\n",
        "nfs = [8,16,32,32]\n",
        "model = get_cnn_model(data, nfs)\n",
        "learn,run = get_runner(model, data, lr=0.4, cbs=cbfs)\n",
        "model"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Sequential(\n",
              "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (1): Sequential(\n",
              "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (2): Sequential(\n",
              "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (3): Sequential(\n",
              "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (4): AdaptiveAvgPool2d(output_size=1)\n",
              "  (5): Lambda()\n",
              "  (6): Linear(in_features=32, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beCjgQizdgXu",
        "colab_type": "code",
        "outputId": "b038e0e1-e590-41fe-fe10-43786e4c46f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "run.fit(3, learn)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train: [1.1092852127976962, tensor(0.3497, device='cuda:0')]\n",
            "valid: [1.09547951643744, tensor(0.3276, device='cuda:0')]\n",
            "train: [1.091418268918954, tensor(0.3496, device='cuda:0')]\n",
            "valid: [1.0856582907463301, tensor(0.3520, device='cuda:0')]\n",
            "train: [1.0810746008016396, tensor(0.3764, device='cuda:0')]\n",
            "valid: [1.0792824322180683, tensor(0.4223, device='cuda:0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6PW9wVz98ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lin(nn.Module):\n",
        "    __constants__ = ['bias', 'in_features', 'out_features']\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(Lin, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, input, new_params=None, new_bias=None):\n",
        "        if new_params is not None and new_bias is not None:\n",
        "            F.linear(input, new_params, new_bias)\n",
        "        return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiCqTDWqd9YU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_x_train = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "toy_y_train = np.array([0, 1, 1, 0])\n",
        "toy_x_valid = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "toy_y_valid = np.array([0, 1, 1, 0])\n",
        "toy_x_train, toy_y_train, toy_x_valid, toy_y_valid = torch.from_numpy(toy_x_train).float(), torch.from_numpy(toy_y_train), torch.from_numpy(toy_x_valid).float(), torch.from_numpy(toy_y_valid)\n",
        "toy_train_ds,toy_valid_ds = Dataset(toy_x_train, toy_y_train),Dataset(toy_x_valid, toy_y_valid)\n",
        "toy_nh, toy_bs = 120, 4\n",
        "toy_c = toy_y_train.max().item()+1\n",
        "loss_func = F.cross_entropy\n",
        "toy_data = DataBunch(*get_dls(toy_train_ds, toy_valid_ds, toy_bs), toy_c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eV9d_eBjd9bY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_y_train = []\n",
        "# for i in range(0, len(y_train)):\n",
        "#     temp = [0, 0, 0]\n",
        "#     temp[y_train[i].item()] = 1\n",
        "#     new_y_train.append(temp)\n",
        "# new_y_train = np.array(new_y_train)\n",
        "# new_y_train = torch.from_numpy(new_y_train)\n",
        "# new_y_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wee7OjBZd9hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_loss(output, targets):\n",
        "#     expand_targets = torch.zeros_like(output)\n",
        "#     for i in range(0, len(expand_targets)): expand_targets[i][targets[i]] = 1\n",
        "#     # return torch.sum((output - expand_targets) * (output - expand_targets)) / (2.0 * output.shape[0])\n",
        "#     # temp = -np.sum(np.nan_to_num(targets) * np.log(1e-15 + output[-1]))\n",
        "#     return -torch.sum(expand_targets * torch.log(output))\n",
        "\n",
        "# def d_loss(output, targets):\n",
        "#     expand_targets = torch.zeros_like(output)\n",
        "#     for i in range(0, len(expand_targets)): expand_targets[i][targets[i]] = 1\n",
        "#     return -(expand_targets) / (output)\n",
        "\n",
        "def d2_loss(output, targets):\n",
        "    # return torch.ones_like(output)\n",
        "    # print(output.shape, targets.shape)\n",
        "    return torch.sum(torch.diag_embed(output, offset=0, dim1=-2, dim2=-1) - \n",
        "                     output.unsqueeze(-1) @ output.unsqueeze(1), 1)\n",
        "    # print(output.unsqueeze(-1).shape, output.unsqueeze(1).shape)\n",
        "    # print(output.shape, targets.shape)\n",
        "    # print(torch.diag_embed(output, offset=0, dim1=-2, dim2=-1))\n",
        "    # print((output.unsqueeze(-1) @ output.unsqueeze(1)).shape)\n",
        "    # print(targets / (output ** 2))\n",
        "    expand_targets = torch.zeros_like(output)\n",
        "    for i in range(0, len(expand_targets)): expand_targets[i][targets[i]] = 1\n",
        "    # print(expand_targets + 1e-8 / ((output) ** 2 + 1e-8))\n",
        "    return (expand_targets / ((output) ** 2 )).t()\n",
        "\n",
        "def make_flat_v1(tensor_list):\n",
        "    temp = []\n",
        "    for c in range(0, len(tensor_list), 2):\n",
        "        temp.append(tensor_list[c].t().flatten())\n",
        "    for c in range(1, len(tensor_list), 2):\n",
        "        temp.append(tensor_list[c].t().flatten())\n",
        "    return torch.cat(temp, dim=0)\n",
        "\n",
        "def make_flat(tensor_list):\n",
        "    temp = []\n",
        "    for c in range(0, len(tensor_list)):\n",
        "        temp.append(tensor_list[c].view(-1))\n",
        "    return torch.cat(temp, dim=0)\n",
        "\n",
        "def calc_accuracy(out, yb): \n",
        "    # print(out.shape, yb.shape)\n",
        "    # print(out[:10])\n",
        "    # print(yb[:10])\n",
        "    return (torch.argmax(out, dim=1)==torch.argmax(yb, dim=1)).float().mean()\n",
        "\n",
        "def inner_product(arr1, arr2):\n",
        "    # if len(arr1) != len(arr2): return None\n",
        "    product = 0.0\n",
        "    for i,j in zip(arr1, arr2):\n",
        "        product += torch.matmul(i.view(-1), j.view(-1))\n",
        "    return product\n",
        "\n",
        "def J_dot(J, vec, transpose_J=False, out=None):\n",
        "    \"\"\"Compute the product of a Jacobian and some vector.\"\"\"\n",
        "\n",
        "    if J.ndim == 2:\n",
        "        # note: the first dimension is the batch, so ndim==2 means\n",
        "        # this is a vector representation\n",
        "        if out is None:\n",
        "            # passing out=None fails for some reason\n",
        "            return torch.mul(J, vec) #np.multiply(J, vec)\n",
        "        else:\n",
        "            return torch.mul(J, vec, out=out) #np.multiply(J, vec, out=out)\n",
        "    else:\n",
        "        if transpose_J:\n",
        "            J = torch.transpose(J, 2, 1) #np.transpose(J, (0, 2, 1))\n",
        "\n",
        "        if out is None:\n",
        "            # passing out=None fails for some reason\n",
        "            return torch.einsum(\"ijk,ik->ij\", J, vec) \n",
        "            #np.einsum(\"ijk,ik->ij\", J, vec)\n",
        "\n",
        "        if out is vec:\n",
        "            tmp_vec = vec.copy()\n",
        "        else:\n",
        "            tmp_vec = vec\n",
        "\n",
        "        return torch.einsum(\"ijk,ik->ij\", J, tmp_vec, out=out) \n",
        "        #np.einsum(\"ijk,ik->ij\", J, tmp_vec, out=out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9NVCP9Ad9jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Optimizer\n",
        "\n",
        "class HessianFree(Optimizer):\n",
        "    \"\"\"Implements Hessian Free Optimisation algorithm.\n",
        "\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        CG_iter (int): maximum number of CG iterations to run per epoch\n",
        "        init_damping (float): the initial value of the Tikhonov damping\n",
        "        net (nn.Module): the network that uses this optimizer\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, CG_iter=250, init_damping=1, net=None):\n",
        "        if not 0.0 <= CG_iter:\n",
        "            raise ValueError(\"Invalid number of CG iterations: {}\".format(cg_iter))\n",
        "        if not 0.0 <= init_damping:\n",
        "            raise ValueError(\"Invalid Tikhonov damping value: {}\".format(init_damping))\n",
        "        if net == None:\n",
        "            raise ValueError(\"Invalid net value: {}\".format(net))\n",
        "        \n",
        "        defaults = dict(CG_iter=CG_iter, damping=init_damping, net=net)\n",
        "        super(HessianFree, self).__init__(params, defaults)\n",
        "        \n",
        "        self.CG_iter = CG_iter\n",
        "        self.damping = init_damping\n",
        "        self.net = net\n",
        "        self.init_delta = None\n",
        "\n",
        "    def params(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                yield p.data\n",
        "\n",
        "    def grads(self, mul=1.0):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                yield p.grad.data * mul\n",
        "\n",
        "    def new_params(self, delta, lr=1.0):\n",
        "        for group in self.param_groups:\n",
        "            for p,d in zip(group['params'], delta):\n",
        "                yield p.data + d * lr\n",
        "\n",
        "    def get_init_delta(self, mul=1.0):\n",
        "        if self.init_delta is None:\n",
        "            for group in self.param_groups:\n",
        "                for p in group['params']:\n",
        "                    yield torch.zeros_like(p, requires_grad=False)\n",
        "        else:\n",
        "            for d in self.init_delta:\n",
        "                yield d * mul\n",
        "\n",
        "    def update_params(self, delta, lr=1.0):\n",
        "        with torch.no_grad():\n",
        "            for group in self.param_groups:\n",
        "                for p, d in zip(group['params'], delta):\n",
        "                    p.add_(lr * d)\n",
        "                    if p.grad is not None:\n",
        "                        p.grad.detach_()\n",
        "                        p.grad.zero_()\n",
        "                \n",
        "    def get_G_dir(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                yield torch.zeros_like(p, requires_grad=False)\n",
        "\n",
        "    def get_residual(self):\n",
        "        for r in self.residual:\n",
        "            yield r\n",
        "\n",
        "    def generator_add(self, gen1, gen2):\n",
        "        for g1,g2 in zip(gen1,gen2):\n",
        "            yield g1 + g2\n",
        "\n",
        "    # def grad_params(self):\n",
        "    #     return [p for pg in self.param_groups\n",
        "    #         for p in pg if p.grad is not None]\n",
        "\n",
        "    # def zero_grad(self):\n",
        "    #     for p in self.grad_params():\n",
        "    #         p.grad.detach_()\n",
        "    #         p.grad.zero_()\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        err = closure()  # note: don't reuse previous error (diff batch)\n",
        "\n",
        "        # compute gradient\n",
        "        # params = []\n",
        "        # grads = []\n",
        "        # for group in self.param_groups:\n",
        "        #     for p in group['params']:\n",
        "        #         params.append(p.data)\n",
        "        #         if p.grad is None:\n",
        "        #             continue\n",
        "        #         grads.append(p.grad.data) \n",
        "        # self.params = params\n",
        "        # print(grads)\n",
        "        # grad = make_flat(grads) #grad = self.net.backward()\n",
        "        params = [p for p in self.params()]\n",
        "        # run CG\n",
        "        if self.init_delta is None:\n",
        "            self.init_delta = [d for d in self.get_init_delta()]\n",
        "            # self.init_delta = [torch.zeros_like(p, requires_grad=False) \n",
        "            #                    for p in self.params()]\n",
        "        deltas = self.conjugate_gradient(self.get_init_delta(mul=0.95), self.grads(), \n",
        "                                         iters=self.CG_iter, params=params)\n",
        "\n",
        "        self.init_delta = deltas[-1][1]  # note: don't backtrack this\n",
        "\n",
        "        # CG backtracking\n",
        "        new_err = np.inf\n",
        "        for j in range(len(deltas) - 1, -1, -1):\n",
        "            # new_params = []\n",
        "            # for group in self.param_groups:\n",
        "            #     for c, p in enumerate(group['params']):\n",
        "            #         new_params.append(p + deltas[j][1][c])\n",
        "            # self.new_params = [self.params[p] + deltas[j][1][p] for p in range(0, len(self.params))]\n",
        "            prev_err = closure(self.new_params(deltas[j][1]))\n",
        "            # prev_err = closure(W + deltas[j][1])\n",
        "            # print(\"prev\", prev_err, \"new\", new_err)\n",
        "            if prev_err > new_err:\n",
        "                break\n",
        "            delta = deltas[j][1]\n",
        "            new_err = prev_err\n",
        "        else:\n",
        "            j -= 1\n",
        "        \n",
        "        # update damping parameter (compare improvement predicted by\n",
        "        # quadratic model to the actual improvement in the error)\n",
        "        quad = (0.5 * inner_product(self.calc_G(delta, damping=self.damping, params=params), \n",
        "                                    delta) + inner_product(self.grads(), delta))\n",
        "        # print(quad.item())\n",
        "        improvement_ratio = ((new_err - err) / quad) if quad != 0 else 1\n",
        "        if improvement_ratio < 0.25:\n",
        "            self.damping *= 1.5\n",
        "        elif improvement_ratio > 0.75:\n",
        "            self.damping *= 0.66\n",
        "        # print(improvement_ratio.item(), quad.item())\n",
        "        # line search to find learning rate\n",
        "        l_rate = 1.0\n",
        "        min_improv = min(1e-2 * inner_product(self.grads(), delta), 0)\n",
        "        \n",
        "        for _ in range(60):\n",
        "            # check if the improvement is greater than the minimum\n",
        "            # improvement we would expect based on the starting gradient\n",
        "            if new_err <= err + l_rate * min_improv:\n",
        "                break\n",
        "            l_rate *= 0.8\n",
        "            # self.new_params = [self.params[p] + (l_rate * delta[p]) for p in range(0, len(self.params))]\n",
        "            # new_params = []\n",
        "            # for group in self.param_groups:\n",
        "            #     for c, p in enumerate(group['params']):\n",
        "            #         new_params.append(p + l_rate * delta[c])\n",
        "            new_err = closure(self.new_params(delta, lr=l_rate))\n",
        "            # print(\"prev_err\", err, \"new_err\", new_err)\n",
        "            # new_err = closure(W + l_rate * delta)\n",
        "        else:\n",
        "            # no good update, so skip this iteration\n",
        "            l_rate = 0.0\n",
        "            new_err = err\n",
        "        self.update_params(delta, lr=l_rate)\n",
        "        # with torch.no_grad():\n",
        "        #     for group in self.param_groups:\n",
        "        #         for c, p in enumerate(group['params']):\n",
        "        #             p += l_rate * delta[c]\n",
        "        #             if p.grad is not None:\n",
        "        #                 p.grad.detach_()\n",
        "        #                 p.grad.zero_()\n",
        "        # print(self.damping)\n",
        "        # assert False\n",
        "        # print(\"deltas len\", len(deltas))\n",
        "        del deltas\n",
        "        return #[l_rate * delta[c] for c in range(0, len(delta))]\n",
        "\n",
        "    def conjugate_gradient(self, init_delta, grad, iters=250, params=params):\n",
        "        \"\"\"Find minimum of quadratic approximation using conjugate gradient\n",
        "        algorithm.\"\"\"\n",
        "\n",
        "        store_iter = 5\n",
        "        store_mult = 1.3\n",
        "        deltas = []\n",
        "        vals = torch.zeros(iters) \n",
        "        base_grad = self.grads(mul=-1.0)\n",
        "        delta = [d for d in init_delta]\n",
        "        # base_grad = [g.mul(-1.0) for g in grad] # note negative, some CG algorithms are flipped\n",
        "        # delta = [d.mul(0.95) for d in init_delta]\n",
        "        # G_dir = [torch.zeros_like(g, requires_grad=False) for g in grad]\n",
        "        G_dir = [g for g in self.get_G_dir()]\n",
        "\n",
        "        residual = [r for r in base_grad]\n",
        "        self.calc_G(delta, damping=self.damping, out=G_dir, params=params)\n",
        "        # for r, g in residual, self.G_dir: r.sub_(g)\n",
        "        for r in range(0, len(residual)): residual[r].sub_(G_dir[r])\n",
        "        res_norm = inner_product(residual, residual)\n",
        "        direction = residual.copy()\n",
        "        # print(direction)\n",
        "        \n",
        "        for i in range(iters):\n",
        "            # calculate step size\n",
        "            self.calc_G(direction, damping=self.damping, out=G_dir, params=params)\n",
        "            \n",
        "            step = res_norm / inner_product(direction, G_dir)\n",
        "            if ((step + 1) == step or (step != step)):\n",
        "                print(\"Non-finite step value (%f)\" % step)\n",
        "                break\n",
        "\n",
        "            # update weight delta\n",
        "            for d in range(0, len(delta)): delta[d].add_(step * direction[d])\n",
        "            \n",
        "            # update residual\n",
        "            for r in range(0, len(residual)): residual[r].sub_(step * G_dir[r])\n",
        "            new_res_norm = inner_product(residual, residual)\n",
        "\n",
        "            if new_res_norm < 1e-20:\n",
        "                # early termination (mainly to prevent numerical errors);\n",
        "                # the main termination condition is below.\n",
        "                break\n",
        "\n",
        "            # update direction\n",
        "            beta = new_res_norm / res_norm\n",
        "            for d in range(0, len(direction)): \n",
        "                direction[d] = beta * direction[d] + residual[d]\n",
        "            # direction *= beta\n",
        "            # direction += residual\n",
        "\n",
        "            res_norm = new_res_norm\n",
        "\n",
        "            # store deltas for backtracking\n",
        "            if i == store_iter:\n",
        "                deltas += [(i, delta.copy())]\n",
        "                store_iter = int(store_iter * store_mult)\n",
        "\n",
        "            # martens termination conditions\n",
        "            vals[i] = -0.5 * inner_product(self.generator_add(residual, base_grad), delta)\n",
        "            # vals[i] = -0.5 * inner_product([residual[r] + base_grad[r] \n",
        "            #                                 for r in range(0, len(residual))], delta)\n",
        "            \n",
        "            gap = max(int(0.1 * i), 10)\n",
        "\n",
        "            if (i > gap and vals[i - gap] < 0 \n",
        "                    and (vals[i] - vals[i - gap]) / vals[i] < 5e-6 * gap):\n",
        "                break\n",
        "\n",
        "        deltas += [(i, delta.copy())]\n",
        "        return deltas\n",
        "\n",
        "    def calc_G(self, v, damping=0, out=None, params=None):\n",
        "        \"\"\"Compute Gauss-Newton matrix-vector product.\"\"\"\n",
        "\n",
        "        if out is None:\n",
        "            Gv = [g for g in self.get_G_dir()]\n",
        "        else:\n",
        "            Gv = out\n",
        "            for val in Gv: val.fill_(0.0)\n",
        "\n",
        "        # R forward pass\n",
        "        R_activations = [torch.zeros_like(a, requires_grad=False) for a in self.net.activations] \n",
        "        for i in range(1, len(R_activations)):\n",
        "            R_activations[i].add_(self.net.activations[(i-1)] @ v[(i-1)*2].t() + v[(i-1)*2 + 1].unsqueeze(0))\n",
        "            R_activations[i].add_(R_activations[(i-1)] @ params[(i-1)*2].t())\n",
        "            R_activations[i].mul_(self.net.d_activations[i])\n",
        "            # R_activations[i] = J_dot(self.net.d_activations[i], R_activations[i])\n",
        "        \n",
        "        # backward pass\n",
        "        R_error = R_activations\n",
        "        for i in range(len(R_error)-1, 0, -1):\n",
        "            if (i == len(R_error)-1): \n",
        "                R_error[i] *= self.net.d2_loss_v\n",
        "            else:\n",
        "                R_error[i].fill_(0.0)\n",
        "\n",
        "            # RDs[i] = RDa[i] * self.net.d_activations[i].t() \n",
        "            # RDW[i] = RDs[i] @ self.net.activations[(i-1)].t()\n",
        "            # RDb[i] = RDs[i]\n",
        "            # RDa[i-1] = params[i*2].t() @ RDs[i]\n",
        "\n",
        "            R_error[i] *= self.net.d_activations[i]\n",
        "            Gv[(i-1)*2] = R_error[i].t() @ self.net.activations[i-1]\n",
        "            Gv[(i-1)*2+1] = torch.sum(R_error[i], 0)\n",
        "            R_error[i-1] += R_error[i] @ params[(i-1)*2]\n",
        "        # print(R_activations[0].shape[0])\n",
        "        for val in Gv: val /= (R_activations[0].shape[0])\n",
        "\n",
        "        for i in range(0, len(Gv)): Gv[i] += damping * v[i]  # Tikhonov damping\n",
        "        return Gv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKjyLqtcd9pP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SequentialModel(nn.Module):\n",
        "    def __init__(self, *layers, shapes=None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "        self.act_means = [[] for _ in layers]\n",
        "        self.act_stds  = [[] for _ in layers]\n",
        "        self.d2_loss_v = None\n",
        "        self.shapes = shapes\n",
        "        \n",
        "    def __call__(self, x, new_params=None):\n",
        "        if new_params is not None:\n",
        "            for i,l in enumerate(self.layers):\n",
        "                if (isinstance(l, type(nn.ReLU())) or isinstance(l, type(nn.Softmax()))):\n",
        "                    x = l(x)\n",
        "                else:\n",
        "                    x @= next(new_params).t() \n",
        "                    x += next(new_params)\n",
        "            return x\n",
        "        self.activations = []\n",
        "        self.d_activations = []\n",
        "        self.activations.append(x)\n",
        "        self.d_activations.append(x == x)\n",
        "        for i,l in enumerate(self.layers):\n",
        "            out_x = l(x)\n",
        "            if (isinstance(l, type(nn.ReLU())) or isinstance(l, type(nn.Softmax()))):\n",
        "                self.activations.append(x)\n",
        "                self.d_activations.append(x == out_x)\n",
        "            x = out_x\n",
        "            out_x = None\n",
        "            assert x is not None\n",
        "            # self.act_means[i].append(x.data.mean())\n",
        "            # self.act_stds [i].append(x.data.std())\n",
        "        return x\n",
        "    \n",
        "    def __iter__(self): return iter(self.layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCORohtNd9sk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(data, lr=0.5, nh=50, CG_iter=500, init_damping=20):\n",
        "    m = data.train_ds.x.shape[1]\n",
        "    shapes = [(m, nh), (nh, data.c)]\n",
        "    model = SequentialModel(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, data.c), nn.Softmax(dim=1), shapes=shapes) \n",
        "    # model = SequentialModel(Lin(m, nh), nn.ReLU(), Lin(nh, data.c), nn.ReLU(), shapes=shapes)\n",
        "    # return model, optim.SGD(model.parameters(), lr=lr) \n",
        "    return model, HessianFree(model.parameters(), CG_iter=CG_iter, init_damping=init_damping, net=model) #optim.SGD(model.parameters(), lr=lr) \n",
        "\n",
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJQ-UCk5d9yl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_learn = Learner(*get_model(toy_data), loss_func, toy_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zymzdNtqd9wQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(*get_model(data, nh=50, CG_iter=250, init_damping=30), loss_func, data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqgP0ONegEF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIFi48Pmd9mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(epochs, learn):\n",
        "    for epoch in range(epochs):\n",
        "        learn.model.train()\n",
        "        for xb,yb in learn.data.train_dl:\n",
        "            y_hat = learn.model(xb)\n",
        "            loss = learn.loss_func(y_hat, yb)\n",
        "            learn.model.d2_loss_v = d2_loss(y_hat, yb)\n",
        "            loss.backward()\n",
        "            # print(get_loss(y_hat, yb))\n",
        "            def closure(new_params=None):\n",
        "                if new_params is not None:\n",
        "                    # y_hat_new = xb.clone().detach()\n",
        "                    # for i in range(0, len(learn.model.layers), 2):\n",
        "                    #     y_hat_new @= next(new_params).t() \n",
        "                    #     y_hat_new += next(new_params)\n",
        "                    #     # y_hat_new = y_hat_new @ new_params[i].t() + new_params[i+1]\n",
        "                    #     y_hat_new.clamp_min_(0.)\n",
        "                    # loss = learn.loss_func(y_hat_new, yb)\n",
        "                    loss = learn.loss_func(learn.model(xb, new_params=new_params), yb)\n",
        "                    # # loss = get_loss(y_hat_new, yb)\n",
        "                    # # print(loss)\n",
        "                    # del y_hat_new\n",
        "                else:\n",
        "                    loss = learn.loss_func(learn.model(xb), yb)\n",
        "                    # loss = get_loss(learn.model(xb), yb)\n",
        "                return loss.item()\n",
        "            learn.opt.step(closure) #learn.opt.step(closure)\n",
        "            learn.opt.zero_grad()\n",
        "        learn.model.eval()\n",
        "        with torch.no_grad():\n",
        "            tot_loss,tot_acc = 0.0, 0.0\n",
        "            for xb,yb in learn.data.valid_dl:\n",
        "                pred = learn.model(xb)\n",
        "                tot_loss += learn.loss_func(pred, yb)\n",
        "                tot_acc  += accuracy (pred,yb)\n",
        "        nv = len(learn.data.valid_dl)\n",
        "        print(epoch, tot_loss/nv, tot_acc/nv)\n",
        "        # collected = gc.collect() \n",
        "        # print(\"Garbage collector: collected %d objects.\" % (collected))\n",
        "        # torch.cuda.empty_cache()\n",
        "    return tot_loss/nv, tot_acc/nv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgKxSfAgd9eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def make_flat_v2(tensor_list):\n",
        "#     temp = []\n",
        "#     for c in range(0, len(tensor_list)):\n",
        "#         temp.append(tensor_list[c].view(-1))\n",
        "#     return torch.cat(temp, dim=0)\n",
        "\n",
        "# def inner_product(tens1, tens2):\n",
        "#     if len(tens1) != len(tens2): return None\n",
        "#     product = 0\n",
        "#     for i in range(0, len(tens1)):\n",
        "#         product += torch.matmul(tens1[i].view(-1), tens2[i].view(-1))\n",
        "#     return product\n",
        "\n",
        "# start = [0, 8, 12, 20, 22]\n",
        "# end = [8, 12, 20, 22, 24]\n",
        "# def get(array, i, shape):\n",
        "#     s = start[i] \n",
        "#     e = end[i]\n",
        "#     return array[s:e].view(shape)\n",
        "\n",
        "# m = torch.tensor(np.arange(22))\n",
        "# print(get(m, 0, (2,4)))\n",
        "# x = torch.tensor([[1,5], [2,6], [3, 7], [4, 8]])\n",
        "# y = torch.tensor([17, 18, 19, 20])\n",
        "# z = torch.tensor([[9, 11, 13,15], [10, 12, 14, 16]])\n",
        "# w = torch.tensor([21, 22])\n",
        "# ar = [x, y, z, w]\n",
        "# shapes = []\n",
        "# for s in ar: \n",
        "#     print(s.view(-1))\n",
        "#     shapes.append(s.shape)\n",
        "#     # print(s.t().shape)\n",
        "\n",
        "# f = make_flat_v2(ar)\n",
        "# # for i,d in enumerate(f):\n",
        "# #     d.reshape(shapes[i])\n",
        "# print(f)\n",
        "# print(make_flat(x), make_flat(z))\n",
        "# # quad = (0.5 * torch.matmul(make_flat(temp_G), flat_delta) + torch.matmul(grad, flat_delta))\n",
        "# torch.matmul(make_flat(x), make_flat(z.t())), inner_product([x], [z])\n",
        "# (make_flat(x) * make_flat(z).t()).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3jEYYY5bUqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.diag_embed(output, offset=0, dim1=-2, dim2=-1) - output.unsqueeze(-1) @ output.unsqueeze(1)\n",
        "# y = torch.tensor([[0], [2], [2]])\n",
        "# y_new = [[0, 0, 0, 0] for temp in y]\n",
        "# for i in range(0, len(y_new)): y_new[i][y[i]] = 1\n",
        "# y_new = torch.tensor(y_new)\n",
        "# x = torch.tensor([[1., 2., 3., 4.], [2, 3, 4, 5], [3, 4, 5, 6]])\n",
        "# bd2 = torch.diag_embed(x, offset=0, dim1=-2, dim2=-1) - x.unsqueeze(-1) @ x.unsqueeze(1)\n",
        "# d1 = torch.diag(x[0]) - x[0].unsqueeze(-1).t() @ x[0].unsqueeze(1)\n",
        "# print(y_new)\n",
        "# print(x.shape, y.shape)\n",
        "# print(y_new / (x ** 2))\n",
        "# y_new = torch.zeros_like(x)\n",
        "# for i in range(0, len(y_new)): y_new[i][y[i]] = 1\n",
        "# print(y_new)\n",
        "# print(y_new / (x ** 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGIPLP0leevb",
        "colab_type": "code",
        "outputId": "63710ccd-469a-41b0-ba9c-6a5823855f2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "fit(20, toy_learn)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.6831) tensor(0.5000)\n",
            "1 tensor(0.6825) tensor(0.5000)\n",
            "2 tensor(0.6816) tensor(0.5000)\n",
            "3 tensor(0.6802) tensor(0.7500)\n",
            "4 tensor(0.6782) tensor(0.7500)\n",
            "5 tensor(0.6750) tensor(0.7500)\n",
            "6 tensor(0.6700) tensor(0.7500)\n",
            "7 tensor(0.6621) tensor(0.7500)\n",
            "8 tensor(0.6491) tensor(0.7500)\n",
            "9 tensor(0.6261) tensor(1.)\n",
            "10 tensor(0.5838) tensor(1.)\n",
            "11 tensor(0.5521) tensor(0.7500)\n",
            "12 tensor(0.5482) tensor(0.7500)\n",
            "13 tensor(0.5131) tensor(1.)\n",
            "14 tensor(0.4891) tensor(1.)\n",
            "15 tensor(0.4700) tensor(1.)\n",
            "16 tensor(0.4525) tensor(1.)\n",
            "17 tensor(0.4324) tensor(1.)\n",
            "18 tensor(0.4072) tensor(1.)\n",
            "19 tensor(0.3934) tensor(1.)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.3934), tensor(1.))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWwAKzeqS0Yw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install xdot\n",
        "# !pip install objgraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30BMjKEs_TLe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f834133b-113e-46ea-c7c2-3dff663bcbc3"
      },
      "source": [
        "fit(10, learn)"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.8643) tensor(0.6755)\n",
            "1 tensor(0.8618) tensor(0.6787)\n",
            "2 tensor(0.8608) tensor(0.6814)\n",
            "3 tensor(0.8593) tensor(0.6827)\n",
            "4 tensor(0.8587) tensor(0.6835)\n",
            "5 tensor(0.8576) tensor(0.6838)\n",
            "6 tensor(0.8566) tensor(0.6861)\n",
            "7 tensor(0.8549) tensor(0.6868)\n",
            "8 tensor(0.8543) tensor(0.6888)\n",
            "9 tensor(0.8531) tensor(0.6879)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8531), tensor(0.6879))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP6rFxG_eezf",
        "colab_type": "code",
        "outputId": "45b83f41-8b08-4635-f989-a64ca99439b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "import objgraph\n",
        "import random\n",
        "import inspect\n",
        "import resource\n",
        "objgraph.show_most_common_types()\n",
        "mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "print(\"Memory usage is: {0} KB\".format(mem))\n",
        "objgraph.show_growth(limit=3)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-8fee11e3b897>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mobjgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mobjgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_most_common_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'objgraph'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTYRebmFAhXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# objgraph.show_refs([learn])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWWC4EPDee7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ar1 = np.array([[0., 0.], [0., 1.], [1., 0.], [1., 1.]])\n",
        "# ar2 = np.array([[1,0], [0,1], [0,1], [1,0]])\n",
        "# pr = np.array([[[0.2,0.7], [0.2,0.1], [0.8,0.9], [0.8,0.5]],\n",
        "#                [[0.3,0.7], [0.2,0.1], [0.8,0.9], [0.8,0.5]],\n",
        "#                [[0.2,0.7], [0.2,0.1], [0.8,0.9], [0.8,0.5]]])\n",
        "# losses = np.sum(np.nan_to_num(pr - ar2) ** 2, axis=tuple(range(1, pr.ndim))) / 2\n",
        "# losses = -np.sum(np.nan_to_num(ar2) * np.log(pr), axis=tuple(range(1, pr.ndim)))\n",
        "# print(losses)\n",
        "# check_loss = np.sum([np.true_divide(np.sum(l), 1) for l in losses if l is not None])\n",
        "# print(check_loss)\n",
        "# new_pr = (torch.tensor([t.flatten() for t in pr]))\n",
        "# print(new_pr.shape)\n",
        "# print(torch.diag(torch.tensor(pr[0].flatten()) * (1 - pr[0].flatten())))\n",
        "# assert (new_pr.unsqueeze(-1) @ new_pr.unsqueeze(1)).shape == torch.diag_embed(new_pr, offset=0, dim1=-2, dim2=-1).shape\n",
        "# testing = torch.sum(torch.diag_embed(new_pr, offset=0, dim1=-2, dim2=-1) - new_pr.unsqueeze(-1) @ new_pr.unsqueeze(1), 0)\n",
        "# print(testing.clamp_min(0.0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYGTPSNlJdyO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc \n",
        "i = 0 \n",
        "  \n",
        "# create a cycle and on each iteration x as a dictionary \n",
        "# assigned to 1 \n",
        "def create_cycle(): \n",
        "    x = { } \n",
        "    x[i+1] = x \n",
        "    print(x)\n",
        "  \n",
        "# lists are cleared whenever a full collection or  \n",
        "# collection of the highest generation (2) is run \n",
        "collected = gc.collect() # or gc.collect(2) \n",
        "print(\"Garbage collector: collected %d objects.\" % (collected))\n",
        "  \n",
        "print(\"Creating cycles...\")\n",
        "for i in range(10): \n",
        "    create_cycle() \n",
        "\n",
        "collected = gc.collect() \n",
        "  \n",
        "print(\"Garbage collector: collected %d objects.\" % (collected))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0eA9JR3SkK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}